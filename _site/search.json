[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bhanu Varaprasad",
    "section": "",
    "text": "I'm Bhanu Varaprasad, experienced With 2.5 years of experience in data analytics at HCL Technologies, I decided to pursue a master's in advanced data analytics at the University of North Text to elevate my skills. My academic journey includes a strong foundation in mathematics, diverse coursework, and hands-on experience in programming languages such as C, and Java, and data analysis tools like Python, R, and Tableau. I actively participated in technical projects, and seminars, and honed my skills in areas like LIMS, ERDS, SQL, HTML, CSS, and JavaScript. As a full-time student, I dedicate my time to assignments, skill enhancement, and fitness. Both professionally and personally, I leverage analytics for personal growth, family business, travel, and healthcare. With a focus on deep learning and AI techniques, I aim to become an expert in data analytics and contribute to top-tier organizations."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my blog. Welcome!\n\nIn this section blog we will review a few of my projects done on different environments and career period."
  },
  {
    "objectID": "index.html#about-this-blog",
    "href": "index.html#about-this-blog",
    "title": "Surya Vardhan",
    "section": "",
    "text": "Welcome to my corner of the internet! I’m Surya Vardhan, a passionate individual deeply engaged in the world of technology and data. With a curiosity-driven mindset, I explore the realms of data science, machine learning, and beyond. Join me on this journey of discovery and innovation!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Bhanu Varaprasad",
    "section": "",
    "text": "I’m Bhagyasri Katuri, a detail-oriented Data Analyst with a year of experience. I’m seeking a part-time, on-campus role at the University of North Texas (UNT) to apply my analytical skills and data interpretation expertise. Currently pursuing an Advanced Data Analytics degree at UNT (Jan 2023 - Dec 2024), my background includes serving as a Data Analyst at EverestDx PVT Limited (Oct 2020 - Dec 2022). I excel in data analysis, use tools like Excel, Java, SQL, Adobe XD, and Illustrator, and have a knack for data visualization and effective communication. I look forward to contributing valuable insights to UNT’s data-driven initiatives."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 12, 2023\n\n\nWine dataset exploratory data analysis\n\n\nBhanu Varaprasad\n\n\n\n\nDec 8, 2023\n\n\nUnveiling the Marvels of Data Science: A Journey into the Heart of Information\n\n\nBhanu Varaprasad\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/postwithplot/index.html",
    "href": "posts/postwithplot/index.html",
    "title": "TidyTuesday: Retail Sales data analysis with Plotly in R",
    "section": "",
    "text": "In this TidyTuesday analysis, I explored retail sales data using R and leveraged the power of Plotly for interactive visualizations. The dataset, likely sourced from the TidyTuesday project, was loaded and examined to gain insights into retail trends. Plotly, a dynamic plotting library, was employed to create engaging and interactive charts, allowing for a deeper understanding of sales patterns, seasonality, or any trends within the retail sector. This approach not only facilitates exploration but also enhances the communicative aspect of data findings through visually appealing and interactive plots.\n\n# Load necessary libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(plotly)\n\nWarning: package 'plotly' was built under R version 4.3.2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(gplots)\n\nWarning: package 'gplots' was built under R version 4.3.2\n\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.2\n\n\nLoading required package: lattice\n\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.2\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(e1071)\n\nWarning: package 'e1071' was built under R version 4.3.2\n\n# Load the Walmart data\nWalmart &lt;- read.csv(\"C:/Users/HP/Downloads/Walmart.csv\")\n\n# Data Cleanup\n# Convert Date column to Date format\nWalmart$Date &lt;- as.Date(Walmart$Date)\n\n# Feature Engineering\n# Extract Year, Month, and Day from Date\nWalmart$Year &lt;- format(Walmart$Date, \"%Y\")\nWalmart$Month &lt;- format(Walmart$Date, \"%m\")\nWalmart$Day &lt;- format(Walmart$Date, \"%d\")\n\n# Exploratory Data Analysis (EDA)\n\n# Display the first few rows of the dataset\nhead(Walmart)\n\n  Store       Date Weekly_Sales Holiday_Flag Temperature Fuel_Price      CPI\n1     1 0005-02-20      1643691            0       42.31      2.572 211.0964\n2     1 0012-02-20      1641957            1       38.51      2.548 211.2422\n3     1 0019-02-20      1611968            0       39.93      2.514 211.2891\n4     1 0026-02-20      1409728            0       46.63      2.561 211.3196\n5     1 0005-03-20      1554807            0       46.50      2.625 211.3501\n6     1 0012-03-20      1439542            0       57.79      2.667 211.3806\n  Unemployment Year Month Day\n1        8.106 0005    02  20\n2        8.106 0012    02  20\n3        8.106 0019    02  20\n4        8.106 0026    02  20\n5        8.106 0005    03  20\n6        8.106 0012    03  20\n\n# Summary statistics\nsummary(Walmart)\n\n     Store         Date             Weekly_Sales      Holiday_Flag    \n Min.   : 1   Min.   :0001-04-20   Min.   : 209986   Min.   :0.00000  \n 1st Qu.:12   1st Qu.:0008-07-20   1st Qu.: 553350   1st Qu.:0.00000  \n Median :23   Median :0016-04-20   Median : 960746   Median :0.00000  \n Mean   :23   Mean   :0016-03-07   Mean   :1046965   Mean   :0.06993  \n 3rd Qu.:34   3rd Qu.:0023-12-20   3rd Qu.:1420159   3rd Qu.:0.00000  \n Max.   :45   Max.   :0031-12-20   Max.   :3818686   Max.   :1.00000  \n  Temperature       Fuel_Price         CPI         Unemployment   \n Min.   : -2.06   Min.   :2.472   Min.   :126.1   Min.   : 3.879  \n 1st Qu.: 47.46   1st Qu.:2.933   1st Qu.:131.7   1st Qu.: 6.891  \n Median : 62.67   Median :3.445   Median :182.6   Median : 7.874  \n Mean   : 60.66   Mean   :3.359   Mean   :171.6   Mean   : 7.999  \n 3rd Qu.: 74.94   3rd Qu.:3.735   3rd Qu.:212.7   3rd Qu.: 8.622  \n Max.   :100.14   Max.   :4.468   Max.   :227.2   Max.   :14.313  \n     Year              Month               Day           \n Length:6435        Length:6435        Length:6435       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n\n# Distribution plot for Weekly Sales\nggplot(Walmart, aes(x = Weekly_Sales)) +\n  geom_histogram(binwidth = 5000, fill = \"#4CAF50\", color = \"#2E7D32\", alpha = 0.7) +\n  labs(title = \"Distribution of Weekly Sales\",\n       x = \"Weekly Sales\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n# Boxplot for Weekly Sales by Store\nggplot(Walmart, aes(x = Store, y = Weekly_Sales)) +\n  geom_boxplot(fill = \"#FF4081\", alpha = 0.7) +\n  labs(title = \"Boxplot of Weekly Sales by Store\",\n       x = \"Store\", y = \"Weekly Sales\") +\n  theme_minimal()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n# Scatter plot for Weekly Sales and Temperature\nggplot(Walmart, aes(x = Temperature, y = Weekly_Sales)) +\n  geom_point(color = \"#1976D2\", alpha = 0.7) +\n  labs(title = \"Scatter Plot of Weekly Sales vs Temperature\",\n       x = \"Temperature\", y = \"Weekly Sales\") +\n  theme_minimal()\n\n\n\n# Quartile plot\nquartile_data &lt;- Walmart %&gt;%\n  group_by(Date) %&gt;%\n  summarise(Q1 = quantile(Weekly_Sales, 0.25),\n            Q3 = quantile(Weekly_Sales, 0.75))\n\nggplot(quartile_data, aes(x = Date)) +\n  geom_line(aes(y = Q1), color = \"#673AB7\", linetype = \"dashed\") +\n  geom_line(aes(y = Q3), color = \"#673AB7\", linetype = \"dashed\") +\n  labs(title = \"Quartile Sales Over Time\",\n       x = \"Date\", y = \"Weekly Sales\",\n       color = \"Quartile\") +\n  theme_minimal()\n\n\n\n# Model Evaluation\n# Split the data into training and testing sets\nset.seed(123)\ntrain_indices &lt;- createDataPartition(Walmart$Weekly_Sales, p = 0.7, list = FALSE)\ntrainData &lt;- Walmart[train_indices, ]\ntestData &lt;- Walmart[-train_indices, ]\n\n# Evaluate the linear regression model\nlm_model &lt;- lm(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)\nlm_predictions &lt;- predict(lm_model, newdata = testData)\nlm_r2 &lt;- cor(lm_predictions, testData$Weekly_Sales)^2\nlm_rmse &lt;- sqrt(mean((lm_predictions - testData$Weekly_Sales)^2))\n\n# Evaluate the random forest model\nrf_model &lt;- randomForest(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)\nrf_predictions &lt;- predict(rf_model, newdata = testData)\nrf_r2 &lt;- cor(rf_predictions, testData$Weekly_Sales)^2\nrf_rmse &lt;- sqrt(mean((rf_predictions - testData$Weekly_Sales)^2))\n\ncat(\"Linear Regression Model:\\n\")\n\nLinear Regression Model:\n\ncat(\"R-squared:\", lm_r2, \"\\n\")\n\nR-squared: 0.0154542 \n\ncat(\"RMSE:\", lm_rmse, \"\\n\\n\")\n\nRMSE: 553801.7 \n\ncat(\"Random Forest Model:\\n\")\n\nRandom Forest Model:\n\ncat(\"R-squared:\", rf_r2, \"\\n\")\n\nR-squared: 0.140237 \n\ncat(\"RMSE:\", rf_rmse, \"\\n\")\n\nRMSE: 521092.6 \n\n\nThe linear regression model achieved a relatively low R-squared value of 0.0155, indicating that the model explains only a small proportion of the variance in the weekly sales. Additionally, the Root Mean Squared Error (RMSE) of 553,801.7 suggests a significant deviation between the predicted and actual sales values, emphasizing the limitations of the linear model in capturing the underlying patterns in the data.\nOn the other hand, the Random Forest model exhibited better performance with a higher R-squared value of 0.1402, signifying a greater ability to explain variability in weekly sales. The lower RMSE of 521,092.6 further supports the Random Forest model as a more accurate predictor compared to the linear regression model. This suggests that the Random Forest algorithm, which leverages ensemble learning and decision trees, provides a more robust and flexible approach for capturing the complex relationships within the Walmart sales data set."
  },
  {
    "objectID": "projects.html#quarto-blog---data-visualizations---animation-and-interactivity",
    "href": "projects.html#quarto-blog---data-visualizations---animation-and-interactivity",
    "title": "Surya Vadhan",
    "section": "",
    "text": "Description 1\n\n\n\nImage 1"
  },
  {
    "objectID": "projects.html#shiny-flex-dashboard---sales-forecasting-and-anomaly-detection",
    "href": "projects.html#shiny-flex-dashboard---sales-forecasting-and-anomaly-detection",
    "title": "Surya Vadhan",
    "section": "Shiny Flex Dashboard - Sales forecasting and anomaly detection",
    "text": "Shiny Flex Dashboard - Sales forecasting and anomaly detection\n\n\nImage 2\n\n\n\nDescription 2"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "posts/b/index.html",
    "href": "posts/b/index.html",
    "title": "TidyTuesday: Retail Sales data analysis with Plotly in R",
    "section": "",
    "text": "In this post, I will analyse the mtcars dataset about Retail Sales …\n\n\nCode\n# Load the mtcars dataset (if not already loaded)\ndata(mtcars)\n\n# Calculate the total sales (number of cars) for each model\nsales_data &lt;- table(mtcars$mpg)\n\n# Create a bar graph\nbarplot(sales_data, \n        main = \"Sales of Cars (mtcars)\",\n        xlab = \"Miles per Gallon (mpg)\",\n        ylab = \"Number of Cars\",\n        col = \"skyblue\",\n        border = \"black\",\n        names.arg = sort(unique(mtcars$mpg)),\n        cex.names = 0.7\n)\n\n# Add a legend\nlegend(\"topright\", legend = \"Car Models\", fill = \"skyblue\", border = \"black\")"
  },
  {
    "objectID": "posts/d/index.html",
    "href": "posts/d/index.html",
    "title": "General R knowledge",
    "section": "",
    "text": "In this group project, you will work with analysts’ forecast data of earning per share (EPS) provided by Wharton Research Data Services (WRDS). Institutional Brokers’ Estimate System (I/B/E/S) provides historical data on certain financial indicators collected from thousands of individual analysts working in more than 3,000 broker houses.\n\nTICKER: A unique identifier assigned to each security. In this group project, you will only model “NFLX” ticker.\nCNAME: Company name\nACTDATS: The Activation date: It is the date when the analyst forecast became effective within the IBES database.\nESTIMATOR: Sellside institution (mostly broker house). It is just the broker.\nANALYS: The person who makes the forecast and work for sellside institution. Estimators and analysts are represented by codes to hide their real names.\nFPI: Forecast Period Indicator: The forecasting period. 6: Next Fiscal Quarter 1: Next Fiscal Year\nMEASURE: The variable being estimated. We have data for earning per share (EPS)\nVALUE: The forecasted value of EPS\nFPEDATS: The Forecast Period End Date: It is the ending date of the fiscal period to which the estimate applies. For the majority of companies, the FPEDATS date is December 31st of that year.\nREVDATS: The Review Date: It is the most recent date on which IBES called the analyst and verified that particular estimate as still valid for that analyst. If an analyst confirms that a previous estimate is still valid, the original database record for that estimate is retained and only the REVDATS variable is updated. If an analyst changes their estimate for a given company, a new record is entered in the database with a new ANNDATS. The old record of the analyst (containing the previous estimate) is retained in the database.\nREVTIMS: Time-stamp of REVDATS\nANNDATS: The Announce date: It is the date on which the analyst first made that particular estimate.\nANNTIMS: Time-stamp of ANNDATS\nACTUAL: The realized EPS, the true EPS value.\nANNDATS_ACT: The Announced date of Actual EPS: The actual EPS value is announced by the company at this date.\nANNTIMS_ACT: The time-stamp of ANNDATS_ACT\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(kableExtra) \n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n# Read CSV file\nNFLX &lt;- read.csv(\"C:/Users/HP/Downloads/NFLX.csv\", header=TRUE)\nnames(NFLX)\n\n [1] \"TICKER\"      \"CNAME\"       \"ACTDATS\"     \"ESTIMATOR\"   \"ANALYS\"     \n [6] \"FPI\"         \"MEASURE\"     \"VALUE\"       \"FPEDATS\"     \"REVDATS\"    \n[11] \"REVTIMS\"     \"ANNDATS\"     \"ANNTIMS\"     \"ACTUAL\"      \"ANNDATS_ACT\"\n[16] \"ANNTIMS_ACT\"\n\n\n\n\nThe first row in NFLX data set: On 09‐Aug-2002 (ANNDATS), analyst 6749 (ANALYS) at Estimator 1872 (ESTIMATOR) predicts that the EPS (MEASURE) for NETFLIX INC. (CNAME) with a ticker of NFLX (TICKER) with forecast period ending 30‐Sep-2002 (FPEDATS) is -$0.0086 (VALUE). This estimates was entered into the database on 12‐Aug-2002 (ACTDATS). On 17-Oct-2002 (ANNDATS_ACT), NETFLIX INC. announced an actual EPS of $7e-04 ($0.0007) (ACTUAL) for this quarter (FPI=6).\n\nhead(NFLX,n=1)\n\n  TICKER        CNAME  ACTDATS ESTIMATOR ANALYS FPI MEASURE   VALUE  FPEDATS\n1   NFLX NETFLIX INC. 20020812      1872   6749   6     EPS -0.0086 20020930\n   REVDATS  REVTIMS  ANNDATS  ANNTIMS ACTUAL ANNDATS_ACT ANNTIMS_ACT\n1 20021018 17:02:56 20020809 14:00:00 -7e-04    20021017    17:04:00\n\n\n\n\n\n\n\n\n\n\n\nTask 1A: Calculate Missingness\n\n\n\nCheck to see the missing values in NFLX dataset and calculate the percent missing for each variable in NFLX and list your findings in R object called NFLX_missingness. NFLX_missingness is a dataframe with two columns: The first column, Variable, stores the variable names and the second column, Missingness shows the percent missing in percentage points with two decimal points.\n\n\n\n\n\n\nlibrary(tidyr)\nNFLX &lt;- read.csv(\"C:/Users/HP/Downloads/NFLX.csv\", header=TRUE)\nnames(NFLX)\n\n [1] \"TICKER\"      \"CNAME\"       \"ACTDATS\"     \"ESTIMATOR\"   \"ANALYS\"     \n [6] \"FPI\"         \"MEASURE\"     \"VALUE\"       \"FPEDATS\"     \"REVDATS\"    \n[11] \"REVTIMS\"     \"ANNDATS\"     \"ANNTIMS\"     \"ACTUAL\"      \"ANNDATS_ACT\"\n[16] \"ANNTIMS_ACT\"\n\n# Calculate the percent missing for each variable in NFLX\nNFLX_missingness &lt;- NFLX %&gt;%\n    summarise_all(~mean(is.na(.)) * 100)\n\n# Print the NFLX_missingness data frame\nprint(NFLX_missingness)\n\n  TICKER CNAME ACTDATS ESTIMATOR ANALYS FPI MEASURE VALUE FPEDATS REVDATS\n1      0     0       0         0      0   0       0     0       0       0\n  REVTIMS ANNDATS ANNTIMS   ACTUAL ANNDATS_ACT ANNTIMS_ACT\n1       0       0       0 4.115304    4.115304           0\n\n\n\n\n\n\n\n\nTask 1B: Data Manipulation\n\n\n\nConduct the following data manipulations on NFLX:\n\nDrop rows from the data set when a variable has a missing value\nDrop rows from the data set the quarterly forecasts (drop FPI=6)\nDeclare TICKER, CNAME, ESTIMATOR , ANALYS, FPI , and MEASURE variables as factor\nDeclare ACTDATS, FPEDATS , ANNDATS, REVDATS, ANNDATS_ACT as time variable.\nDrop ANNTIMS_ACT, ANNTIMS , and REVTIMS\nCreate a new column named YEAR that captures the year in FPEDATS\nName your reduced dataset as NFLX1\nPrint out data structure and the summary of NFLX1\n\n\n\n\n\n\n\n# Copy NFLX to NFLX1 without assigning data types\nNFLX1 &lt;- NFLX\n\n# Drop rows from the data set when a variable has a missing value\nNFLX1 &lt;- NFLX1 %&gt;% na.omit()\n\n# Drop rows from the data set where FPI=6\nNFLX1 &lt;- NFLX1 %&gt;% filter(FPI != 6)\n\n# Drop ANNTIMS_ACT, ANNTIMS, and REVTIMS\nNFLX1 &lt;- NFLX1 %&gt;% select(-ANNTIMS_ACT, -ANNTIMS, -REVTIMS)\n\n# Create a new column named YEAR that is an exact copy of the data in FPEDATS\nNFLX1 &lt;- NFLX1 %&gt;% mutate(YEAR = FPEDATS)\n\n# Print out data structure and the summary of NFLX1\nstr(NFLX1)\n\n'data.frame':   2603 obs. of  14 variables:\n $ TICKER     : chr  \"NFLX\" \"NFLX\" \"NFLX\" \"NFLX\" ...\n $ CNAME      : chr  \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" ...\n $ ACTDATS    : int  20020805 20021202 20021202 20021202 20021205 20030106 20030115 20030116 20030121 20030314 ...\n $ ESTIMATOR  : int  183 2178 1872 220 2178 1872 2227 220 1872 481 ...\n $ ANALYS     : int  79868 80485 6749 57596 80485 6749 82629 57596 6749 81599 ...\n $ FPI        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ MEASURE    : chr  \"EPS\" \"EPS\" \"EPS\" \"EPS\" ...\n $ VALUE      : num  -0.025 -0.0321 -0.0207 -0.0179 -0.0286 -0.0136 -0.0164 -0.0071 0.0107 0.0129 ...\n $ FPEDATS    : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n $ REVDATS    : int  20021129 20021202 20021202 20021206 20021205 20030114 20030115 20030417 20030402 20030409 ...\n $ ANNDATS    : int  20020805 20021202 20021202 20021202 20021204 20030102 20030115 20030116 20030116 20030314 ...\n $ ACTUAL     : num  -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 0.0393 0.0393 0.0393 ...\n $ ANNDATS_ACT: int  20030115 20030115 20030115 20030115 20030115 20030115 20030115 20040121 20040121 20040121 ...\n $ YEAR       : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:217] 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 ...\n  ..- attr(*, \"names\")= chr [1:217] \"5057\" \"5058\" \"5059\" \"5060\" ...\n\nsummary(NFLX1)\n\n    TICKER             CNAME              ACTDATS           ESTIMATOR   \n Length:2603        Length:2603        Min.   :20020805   Min.   :  11  \n Class :character   Class :character   1st Qu.:20101021   1st Qu.: 192  \n Mode  :character   Mode  :character   Median :20141009   Median : 899  \n                                       Mean   :20136831   Mean   :1376  \n                                       3rd Qu.:20180122   3rd Qu.:2502  \n                                       Max.   :20210119   Max.   :4439  \n     ANALYS            FPI      MEASURE              VALUE       \n Min.   :  1047   Min.   :1   Length:2603        Min.   :-0.150  \n 1st Qu.: 71755   1st Qu.:1   Class :character   1st Qu.: 0.190  \n Median : 82010   Median :1   Mode  :character   Median : 0.430  \n Mean   : 89534   Mean   :1                      Mean   : 1.339  \n 3rd Qu.:114459   3rd Qu.:1                      3rd Qu.: 2.015  \n Max.   :194536   Max.   :1                      Max.   : 7.670  \n    FPEDATS            REVDATS            ANNDATS             ACTUAL      \n Min.   :20021231   Min.   :20021129   Min.   :20020805   Min.   :-0.005  \n 1st Qu.:20101231   1st Qu.:20110120   1st Qu.:20101021   1st Qu.: 0.250  \n Median :20141231   Median :20141013   Median :20141009   Median : 0.430  \n Mean   :20137082   Mean   :20137740   Mean   :20136830   Mean   : 1.384  \n 3rd Qu.:20181231   3rd Qu.:20180122   3rd Qu.:20180122   3rd Qu.: 2.680  \n Max.   :20201231   Max.   :20210119   Max.   :20210119   Max.   : 6.080  \n  ANNDATS_ACT            YEAR         \n Min.   :20030115   Min.   :20021231  \n 1st Qu.:20110126   1st Qu.:20101231  \n Median :20150120   Median :20141231  \n Mean   :20145973   Mean   :20137082  \n 3rd Qu.:20190117   3rd Qu.:20181231  \n Max.   :20210119   Max.   :20201231  \n\n\n\n\n\n\n\n\nTask 2: Calculate Number of Analysts and Brokerage Houses\n\n\n\n\nCalculate the total number of unique analysts in NFLX1 dataset that provide forecasts each year and name your R object as NumberAnalyst\nCalculate the total number of unique brokerage houses (ESTIMATOR) in NFLX1 dataset that provide forecasts each year and name your R object as NumberBrokerage\nNeed Written Response in this callout: In which year(s) we have the highest number of unique analysts providing forecasts for NFLX ticker? In which year(s), we have the highest number of unique brokerage houses providing forecasts for the NFLX ticker.\n\n2011 is the year number of distinct analysts providing forecasts for netflix.\n\n\n\n\n\n\n# Calculate the total number of unique analysts providing forecasts each year\nNumberAnalyst &lt;- NFLX1 %&gt;%\n  group_by(YEAR) %&gt;%\n  distinct(ESTIMATOR) %&gt;%\n  summarise(NumAnalysts = n_distinct(ESTIMATOR))\n\n# Print the NumberAnalyst object\nprint(NumberAnalyst)\n\n# A tibble: 19 × 2\n       YEAR NumAnalysts\n      &lt;int&gt;       &lt;int&gt;\n 1 20021231           5\n 2 20031231           8\n 3 20041231          18\n 4 20051231          17\n 5 20061231          19\n 6 20071231          18\n 7 20081231          21\n 8 20091231          32\n 9 20101231          38\n10 20111231          35\n11 20121231          36\n12 20131231          43\n13 20141231          40\n14 20151231          46\n15 20161231          45\n16 20171231          49\n17 20181231          54\n18 20191231          43\n19 20201231          44\n\n# Calculate the total number of unique brokerage houses providing forecasts each year\nNumberBrokerage &lt;- NFLX1 %&gt;%\n  group_by(YEAR) %&gt;%\n  distinct(ESTIMATOR) %&gt;%\n  summarise(NumBrokerage = n_distinct(ESTIMATOR))\n\n# Print the NumberBrokerage object\nprint(NumberBrokerage)\n\n# A tibble: 19 × 2\n       YEAR NumBrokerage\n      &lt;int&gt;        &lt;int&gt;\n 1 20021231            5\n 2 20031231            8\n 3 20041231           18\n 4 20051231           17\n 5 20061231           19\n 6 20071231           18\n 7 20081231           21\n 8 20091231           32\n 9 20101231           38\n10 20111231           35\n11 20121231           36\n12 20131231           43\n13 20141231           40\n14 20151231           46\n15 20161231           45\n16 20171231           49\n17 20181231           54\n18 20191231           43\n19 20201231           44\n\n\n\n\n\n\n\n\nTask 3: Get the most recent forecast in each year\n\n\n\n\nIt is quite possible that an analyst makes multiple forecasts throughout the year for the same fiscal period. Remove observations from NFLX1 if an analyst has multiple predictions for the same year and keep the last one (the most recent forecast for each year). Name your new dataset as NFLX2. This step is crucial for successful execution of the following tasks. Print the dimension of NFLX2.\nCheck your work: If your NFLX2 dataset has 641 rows and 14 columns, then you are on the right track. If not, please seek help!\n\n\n\n\n\n\n\n# Task 3: Get the most recent forecast in each year\n\n# Get the most recent forecast for each analyst in each year\nNFLX2 &lt;- NFLX1 %&gt;%\n    # Group by analyst and year\n    group_by(ANALYS, YEAR) %&gt;%\n    # Filter to keep the most recent forecast for each analyst in each year\n    filter(\n        REVDATS == max(REVDATS)\n    ) %&gt;%\n    # Ungroup the data frame\n    ungroup()\n\n# Print the dimension of NFLX2\ndim(NFLX2)\n\n[1] 641  14\n\n# Check your work: If your NFLX2 dataset has 641 rows and 14 columns, then you are on the right track. If not, please seek help!\n\n\n\n\n\n\n\n\n\nTask 4: Calculate past accuracy\n\n\n\n\nCreate a copy of NFLX2 and call it NFLX3\nFor every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy. In the calculation of forecast performance, you can use the VALUE-ACTUAL as the forecast accuracy measure.\nFor each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy\nAs an example, consider the year 2006, where analyst 1047, employed at brokerage house 464, provided an estimated end-of-period EPS of 0.0929 (VALUE). However, the actual EPS for that year turned out to be 0.1014 (ACTUAL), resulting in a forecasting error of -0.0085. Consequently, in the subsequent year, 2007, the past_accuracy metric for analyst 1047 would reflect this error by taking the value of -0.0085 (VALUE-ACTUAL).\nThis action will create some missing values and this is perfectly fine.\nIf your code produces 144 NAs, then you are on the right track.\nNote that we are creating copies of the original dataset at each step to facilitate error detection in case any mistakes occur during the process.\n\n\n\n\n\n\n\n\n# Create a copy of NFLX2 and call it NFLX3\nNFLX3 &lt;- NFLX2\n\n# Task 4: Calculate past accuracy\n\n# For every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy.\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ANALYS) %&gt;%\n  mutate(accuracy = VALUE - ACTUAL)\n\n# For each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  arrange(YEAR) %&gt;%\n  mutate(past_accuracy = lag(accuracy))\n\n# Check if the code produces 144 NAs\nsum(is.na(NFLX3$past_accuracy))\n\n[1] 144\n\n\n\n\n\n\n\n\n\n\nTask 5: Forecast Horizon\n\n\n\n\nThe longer the forecast horizon, the higher the uncertainty associated with EPS forecasts. To control for this fact, create a new column in NFLX3 called horizon that captures the forecast horizon (ANNDATS_ACT- ANNDATS) for each analyst.\nWe anticipate observing a negative correlation between accuracy and horizon. Typically, as the forecast horizon increases, the accuracy tends to decrease, and vice versa. However, in our dataset, there is an exception where we find a positive correlation between accuracy and horizon for one specific year. Write an R code to identify and determine which year exhibits this positive correlation.\nNeed Written Response in this callout: Enter the year in here.\n2011 2012 2103 2015 and 2018 with 2018 having a forecast value of 0.24.\n\n\n\n\n\n\n\n\n# Task 5: Forecast Horizon\n# Calculate past accuracy and forecast horizon\nNFLX3 &lt;- NFLX3 %&gt;% group_by(ANALYS) %&gt;% arrange(YEAR) %&gt;% mutate(past_accuracy = lag(accuracy, default = NA))\nNFLX3 &lt;- NFLX3 %&gt;% mutate(horizon = as.numeric(difftime(ANNDATS_ACT, ANNDATS, units = \"days\")))\n\n# Calculate correlation and find positive correlation year\ncorrelation_by_year &lt;- NFLX3 %&gt;% group_by(YEAR) %&gt;% summarise(correlation = cor(accuracy, horizon, use = \"complete.obs\"))\npositive_corr_year &lt;- correlation_by_year %&gt;% filter(correlation &gt; 0)\n\n# Print positive correlation year with correlation values\npositive_corr_year %&gt;% mutate(correlation = round(correlation, 2)) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = \"striped\")\n\n\n\n\nYEAR\ncorrelation\n\n\n\n\n20111231\n0.23\n\n\n20121231\n0.07\n\n\n20131231\n0.02\n\n\n20151231\n0.06\n\n\n20181231\n0.24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 6: Experience\n\n\n\n\nWe assume that if an analyst is monitoring a company for a long period of time, he/she is expected to make more informed predictions. Create a new column in NFLX3 called experience that counts the cumulative number of years the analyst monitor (have predictions) the company. Print the summary of experience column.\nHint: Try to use cumsum() function in R.\nNeed Written Response in this callout: Which analyst (s) has the highest number of experience in NFLX3 dataset and for how long do they monitor the NFLX ticker?\n\nAnalyst 72088 and 77748 have 17 years of hardcore experience\n\n\n\n\n\n\n\n # Task 6: Experience\n\n# Calculate the cumulative experience for each analyst\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  mutate(experience = cumsum(!duplicated(YEAR)))\n\n# Find the analyst(s) with the highest experience\nmax_experience &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  summarise(experience = max(experience)) %&gt;%\n  filter(experience == max(experience))\n\n# Print the summary of the experience column\nprint(summary(NFLX3$experience))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.192   6.000  17.000 \n\n# Find the analyst(s) with the highest experience\nmax_experience &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  summarise(experience = max(experience)) %&gt;%\n  filter(experience == max(experience))\n\n# Print the summary of the experience column\nprint(summary(NFLX3$experience))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.192   6.000  17.000 \n\n# Print the analyst(s) with the highest experience\nprint(max_experience)\n\n# A tibble: 2 × 2\n  ANALYS experience\n   &lt;int&gt;      &lt;int&gt;\n1  72088         17\n2  77748         17\n\n\n\n\n\n\n\n\n\n\nTask 7: Size\n\n\n\n\nIf a brokerage house has multiple analysts providing predictions for the same company, it may indicate a greater allocation of resources for company analysis. To capture this, create a new column in the NFLX3 dataset called size that calculates the total count of unique analysts employed per year by each brokerage house (ESTIMATOR)\nNeed Written Response in this callout: Print the frequencies for size variable. What does this frequency table reveal about the distribution of the number of analysts hired by brokerage houses in this dataset?\n\nFrom the table we can conclude that as the number of analysts increase the frequency of hiring reduces exponentially. This indicates that a brokerage preferred hiring one analyst per every season.\n\n\n\n\n\n\n\n # Task 7: Size\n\n# Calculate the total count of unique analysts employed per year by each brokerage house\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ESTIMATOR) %&gt;%\n  mutate(size = n_distinct(ANALYS))\n\n# Print the frequencies for the size variable\nsize_freq &lt;- table(NFLX3$size)\nprint(size_freq)\n\n\n  1   2   3 \n560  72   9 \n\n# Create a frequency table for better visualization\nsize_table &lt;- as.data.frame(size_freq)\ncolnames(size_table) &lt;- c(\"Number of Analysts\", \"Frequency\")\n\n# Sort the table by frequency in descending order\nsize_table &lt;- size_table[order(-size_table$Frequency), ]\n\n# Print the sorted frequency table\nprint(size_table)\n\n  Number of Analysts Frequency\n1                  1       560\n2                  2        72\n3                  3         9\n\n# Summary statistics for size variable\nsummary(NFLX3$size)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.00    1.00    1.14    1.00    3.00 \n\n\n\n\n\n\n\n\n\n\nTask 8: Prediction 1\n\n\n\n\nIn the year 2020, NETFLIX reported an actual earnings per share (EPS) of $6.08. To predict this EPS value based on historical data, we will employ a linear regression model using the dataset NFLX3 up until the year 2019. In this model, the target variable will be ACTUAL and the predictor variables will include VALUE and past_accuracy. C.all your model as model1.\nNeed Written Response in this callout: Using the linear regression model ‘model1,’ which has been trained on historical data up to the year 2019, what is the forecasted EPS (Earnings Per Share) for the year 2020? Please provide a brief explanation of the method you employed to make this prediction. If you encountered any challenges or were unable to make the calculation, briefly describe the specific issues you encountered.\n\nUsing the linear regression model 'model1,' trained on historical data up to the year 2019, we can forecast the EPS (Earnings Per Share) for the year 2020. The method employed involves calculating the R-squared value of the model, which measures the goodness of fit. If the R-squared value is high (greater than 0.5 in this case), we proceed with forecasting.\nIn this example, the code calculates the mean of the 'past_accuracy' variable. If the R-squared value is satisfactory, we generate a forecast for the future period. We create a new data frame for the future period with values for the independent variables (VALUE and past_accuracy). The code then uses the 'predict' function to estimate the EPS for the future period.\nIf the R-squared value is low, a warning message is provided, indicating that the model may not accurately predict future EPS.\nThe method allows us to make forecasts if the model's fit is deemed appropriate. However, we encountered issues with data configuration or model training, those specific challenges should be addressed to ensure the accuracy of the model\n\n\n\n\n\n\n\n# Calculate the mean of past_accuracy\nmean_past_accuracy &lt;- mean(NFLX3$past_accuracy, na.rm = TRUE)\n\n# Convert the 'YEAR' column to the desired format\nNFLX3$YEAR &lt;- as.POSIXct(NFLX3$YEAR, format = \"%Y-%m-%d %H:%M:%S\")\n\n# Create the linear regression model using historical data up to the year 2019\nmodel1 &lt;- lm(ACTUAL ~ VALUE + past_accuracy, data = NFLX3)\n\n# Get the R-squared value of the model\nr_squared &lt;- summary(model1)$r.squared\n\n# If the R-squared value is high, then we can use the model to generate a forecast\nif (r_squared &gt; 0.5) {\n  # Create a new data frame for the future period with the values of the independent variables\n  new_data_future &lt;- data.frame(\n    VALUE = 6.08, # Replace this with the actual value of VALUE for the future period\n    past_accuracy = mean_past_accuracy\n  )\n\n  # Predict the EPS for the future period\n  predicted_eps_future &lt;- predict(model1, newdata = new_data_future)\n\n  # Print the forecasted EPS for the future period\n  cat(\"Forecasted EPS for future period: $\", round(predicted_eps_future, 2))\n} else {\n  # Print a warning message\n  cat(\"The R-squared value is low, so the model may not be able to accurately predict future values of the dependent variable.\")\n}\n\nForecasted EPS for future period: $ 6.3\n\n# Print the mean of past_accuracy\ncat(\"Mean past_accuracy: \", round(mean_past_accuracy, 2))\n\nMean past_accuracy:  -0.09\n\n\n\n\n\n\n\n\n\n\nTask 9: Prediction 2\n\n\n\n\nAs an alternative approach, instead of modeling the ‘ACTUAL’ value, we can obtain the mean and median forecasts for the year 2020 as our best estimates of the EPS value for that year.\nNeed Written Response in this callout: Please calculate these forecasts and then compare them with the results from the previous task. Finally, provide your insights and comments based on your findings.\n\nIn this alternative approach, we opted to calculate the mean and median forecasts for the year 2020 as our best estimates of the EPS value. The mean forecast for 2020 is approximately $1.24, while the median forecast is significantly lower, at approximately $0.41. When comparing these results with the linear regression model from the previous task, it's evident that the model-based forecast might provide a more detailed and potentially accurate prediction. However, these two approaches have their unique merits and drawbacks. The model-driven forecast takes into account historical relationships and variables like 'past_accuracy,' but it heavily relies on the quality of the model fit, as indicated by the R-squared value. On the other hand, the mean and median forecasts provide simple summary statistics but might lack the predictive power of a well-fitted model. The choice between these methods should be influenced by the data quality and the context of the analysis.\n\n\n\n\n\n\n\n# Calculate the mean forecast\nmean_forecast &lt;- mean(NFLX3$VALUE, na.rm = TRUE)\n\n# Calculate the median forecast\nmedian_forecast &lt;- median(NFLX3$VALUE, na.rm = TRUE)\n\n# Print the mean and median forecasts\ncat(\"Mean forecast for 2020: $\", round(mean_forecast, 2))\n\nMean forecast for 2020: $ 1.24\n\ncat(\"Median forecast for 2020: $\", round(median_forecast, 2))\n\nMedian forecast for 2020: $ 0.41\n\n\n\n\n\n\n\n\n\n\nTask 10: Averages\n\n\n\n\nGenerate a new dataset named NFLX4 by aggregating data from NFLX3 Include the variables size, experience, horizon, accuracy, past_accuracy, and ACTUAL in NFLX4. When calculating the yearly averages for these variables, ignore any missing values (NAs). Present a summary of the NFLX4 dataset.\nNeed Written Response in this callout: Subsequently, employ correlation analysis or exploratory data analysis to get insights into the relationships between these variables and ‘ACTUAL,’ if such relationships exist.\n\nAfter conducting correlation analysis and exploratory data analysis on the dataset NFLX4, several interesting insights into the relationships between variables and 'ACTUAL' have emerged\nThe correlation matrix reveals that 'ACTUAL' is positively correlated with 'size' (0.18) and 'experience' (0.69), indicating that, on average, larger groups of analysts and analysts with more experience tend to provide more accurate forecasts for the 'ACTUAL' earnings per share.\nConversely, 'ACTUAL' is negatively correlated with 'horizon' (-0.63) and 'past_accuracy' (-0.80). This suggests that analysts with a longer forecasting horizon and those who have made more accurate predictions in the past tend to have less accurate forecasts for 'ACTUAL' earnings per share.\nIn the exploratory data analysis, scatter plots further highlight these relationships. For instance, the scatter plot of 'ACTUAL' against 'size' shows a general trend of improved accuracy as the number of analysts increases.\nThese findings provide valuable insights for understanding the factors that influence the accuracy of earnings per share forecasts ('ACTUAL'). Larger analyst groups and more experienced analysts tend to provide more accurate forecasts, while longer forecasting horizons and a history of past accuracy can lead to less accurate predictions. These relationships can be crucial for analysts, investors, and decision-makers in the financial domain.\n\n\n\n# Aggregate data and calculate yearly averages, ignoring missing values (NAs)\nNFLX4 &lt;- NFLX3 %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(\n    size = mean(size, na.rm = TRUE),\n    experience = mean(experience, na.rm = TRUE),\n    horizon = mean(horizon, na.rm = TRUE),\n    accuracy = mean(accuracy, na.rm = TRUE),\n    past_accuracy = mean(past_accuracy, na.rm = TRUE),\n    ACTUAL = mean(ACTUAL, na.rm = TRUE)\n  )\n\n# Present a summary of the NFLX4 dataset\nsummary(NFLX4)\n\n      YEAR                          size         experience   \n Min.   :1970-08-20 20:27:11   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1970-08-21 08:57:11   1st Qu.:1.074   1st Qu.:2.664  \n Median :1970-08-21 21:27:11   Median :1.105   Median :3.400  \n Mean   :1970-08-21 21:27:11   Mean   :1.132   Mean   :3.611  \n 3rd Qu.:1970-08-22 09:57:11   3rd Qu.:1.202   3rd Qu.:4.869  \n Max.   :1970-08-22 22:27:11   Max.   :1.300   Max.   :6.061  \n                                                              \n    horizon           accuracy         past_accuracy           ACTUAL       \n Min.   :0.06284   Min.   :-0.822085   Min.   :-0.798219   Min.   :-0.0050  \n 1st Qu.:0.08547   1st Qu.:-0.019087   1st Qu.:-0.028736   1st Qu.: 0.0914  \n Median :0.09289   Median :-0.015035   Median :-0.013423   Median : 0.2643  \n Mean   :0.09004   Mean   :-0.048310   Mean   :-0.060652   Mean   : 0.9248  \n 3rd Qu.:0.09512   3rd Qu.:-0.005415   3rd Qu.:-0.009260   3rd Qu.: 0.5678  \n Max.   :0.10656   Max.   : 0.121449   Max.   :-0.001547   Max.   : 6.0800  \n                                       NA's   :1                            \n\n# Correlation analysis\ncorrelation_matrix &lt;- cor(NFLX4[, c(\"size\", \"experience\", \"horizon\", \"accuracy\", \"past_accuracy\", \"ACTUAL\")], use = \"complete.obs\")\n\n# Print correlation matrix\nprint(correlation_matrix)\n\n                     size  experience    horizon    accuracy past_accuracy\nsize           1.00000000  0.07451284 -0.1317823 -0.04537307    -0.1810330\nexperience     0.07451284  1.00000000 -0.4844637 -0.25882136    -0.4620906\nhorizon       -0.13178225 -0.48446371  1.0000000  0.22264895     0.4979377\naccuracy      -0.04537307 -0.25882136  0.2226489  1.00000000    -0.1604379\npast_accuracy -0.18103301 -0.46209061  0.4979377 -0.16043792     1.0000000\nACTUAL         0.18223220  0.68707354 -0.6346966 -0.31928984    -0.7958850\n                  ACTUAL\nsize           0.1822322\nexperience     0.6870735\nhorizon       -0.6346966\naccuracy      -0.3192898\npast_accuracy -0.7958850\nACTUAL         1.0000000\n\n# Exploratory data analysis\n# Create scatter plots to explore relationships\n\n# Scatter plot of ACTUAL vs. size\nggplot(NFLX4, aes(x = size, y = ACTUAL)) +\n  geom_point(color = \"blue\", shape = 1) +\n  ggtitle(\"ACTUAL vs. size\") +\n  xlab(\"size\") +\n  ylab(\"ACTUAL\")\n\n\n\n# Scatter plot of ACTUAL vs. experience\nggplot(NFLX4, aes(x = experience, y = ACTUAL)) +\n  geom_point(color = \"green\", shape = 2) +\n  ggtitle(\"ACTUAL vs. experience\") +\n  xlab(\"experience\") +\n  ylab(\"ACTUAL\")\n\n\n\n# Scatter plot of ACTUAL vs. horizon\nggplot(NFLX4, aes(x = horizon, y = ACTUAL)) +\n  geom_point(color = \"red\", shape = 3) +\n  ggtitle(\"ACTUAL vs. horizon\") +\n  xlab(\"horizon\") +\n  ylab(\"ACTUAL\")\n\n\n\n# Scatter plot of ACTUAL vs. accuracy\nggplot(NFLX4, aes(x = accuracy, y = ACTUAL)) +\n  geom_point(color = \"orange\", shape = 4) +\n  ggtitle(\"ACTUAL vs. accuracy\") +\n  xlab(\"accuracy\") +\n  ylab(\"ACTUAL\")\n\n\n\n# Scatter plot of ACTUAL vs. past_accuracy\nggplot(NFLX4, aes(x = past_accuracy, y = ACTUAL)) +\n  geom_point(color = \"purple\", shape = 5) +\n  ggtitle(\"ACTUAL vs. past_accuracy\") +\n  xlab(\"past_accuracy\") +\n  ylab(\"ACTUAL\")\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n# Save NFLX4 to a CSV file if needed\n# write.csv(NFLX4, \"NFLX4.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/c/index.html",
    "href": "posts/c/index.html",
    "title": "Exploratory data analysis and machine learning on Iris dataset",
    "section": "",
    "text": "The script loads and explores the Iris dataset, visualizing features like sepal length and petal dimensions. It then implements a decision tree model, assesses its accuracy on a test set, and presents a confusion matrix. The script provides insights through EDA and demonstrates a basic machine learning approach for species classification in the Iris dataset.\n\n\nCode\n# Assuming IRSI_data is your dataset\n# You need to replace it with the actual name of your dataset\n\n# Load necessary libraries\nlibrary(dplyr)\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nCode\n# Load your IRSI data (replace \"your_dataset.csv\" with your actual dataset file)\nIRSI_data &lt;- read.csv(\"C:/Users/HP/Downloads/Iris.csv\")\n\n# Display the first few rows of the dataset\nhead(IRSI_data)\n\n\n  Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm     Species\n1  1           5.1          3.5           1.4          0.2 Iris-setosa\n2  2           4.9          3.0           1.4          0.2 Iris-setosa\n3  3           4.7          3.2           1.3          0.2 Iris-setosa\n4  4           4.6          3.1           1.5          0.2 Iris-setosa\n5  5           5.0          3.6           1.4          0.2 Iris-setosa\n6  6           5.4          3.9           1.7          0.4 Iris-setosa\n\n\nCode\n# Summary statistics\nsummary(IRSI_data)\n\n\n       Id         SepalLengthCm    SepalWidthCm   PetalLengthCm  \n Min.   :  1.00   Min.   :4.300   Min.   :2.000   Min.   :1.000  \n 1st Qu.: 38.25   1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600  \n Median : 75.50   Median :5.800   Median :3.000   Median :4.350  \n Mean   : 75.50   Mean   :5.843   Mean   :3.054   Mean   :3.759  \n 3rd Qu.:112.75   3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100  \n Max.   :150.00   Max.   :7.900   Max.   :4.400   Max.   :6.900  \n  PetalWidthCm     Species         \n Min.   :0.100   Length:150        \n 1st Qu.:0.300   Class :character  \n Median :1.300   Mode  :character  \n Mean   :1.199                     \n 3rd Qu.:1.800                     \n Max.   :2.500                     \n\n\nCode\n# Distribution plot for Sepal.Length\nggplot(IRSI_data, aes(x = SepalLengthCm)) +\n  geom_histogram(binwidth = 0.1, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Sepal Length\",\n       x = \"Sepal Length\",\n       y = \"Frequency\")\n\n\n\n\n\nCode\n# Boxplot for Sepal.Length by Species\nggplot(IRSI_data, aes(x = Species, y = SepalWidthCm)) +\n  geom_boxplot(fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Boxplot of Sepal Length by Species\",\n       x = \"Species\",\n       y = \"Sepal Length\")\n\n\n\n\n\nCode\n# Scatter plot for Petal.Length and Petal.Width\nggplot(IRSI_data, aes(x = PetalLengthCm, y = PetalWidthCm, color = Species)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of Petal Length vs Petal Width\",\n       x = \"Petal Length\",\n       y = \"Petal Width\",\n       color = \"Species\")\n\n\n\n\n\nCode\n# Pair plot\npairs(IRSI_data[, c(\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\")], \n      main = \"Pair Plot of Iris Data\",\n      pch = 21, bg = c(\"red\", \"green3\", \"blue\")[unclass(IRSI_data$Species)])\n\n\n\n\n\nCode\n# Correlation matrix\ncor_matrix &lt;- cor(IRSI_data[, c(\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\")])\n\n# Violin plot\nggplot(IRSI_data, aes(x = Species, y = PetalLengthCm, fill = Species)) +\n  geom_violin(trim = FALSE) +\n  labs(title = \"Violin Plot of Petal Length by Species\",\n       x = \"Species\",\n       y = \"Petal Length\",\n       fill = \"Species\")\n\n\n\n\n\nCode\n# Density plot\nggplot(IRSI_data, aes(x = PetalWidthCm, fill = Species)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Density Plot of Petal Width by Species\",\n       x = \"Petal Width\",\n       fill = \"Species\")\n\n\n\n\n\nCode\n# Load necessary library for machine learning\nlibrary(rpart)\n\n\nWarning: package 'rpart' was built under R version 4.3.2\n\n\nCode\n# Split the data into training and testing sets\nset.seed(123)\nsample_index &lt;- sample(1:nrow(IRSI_data), 0.7 * nrow(IRSI_data))\ntrain_data &lt;- IRSI_data[sample_index, ]\ntest_data &lt;- IRSI_data[-sample_index, ]\n\n# Build a decision tree model\niris_model &lt;- rpart(Species ~ SepalLengthCm + SepalWidthCm + PetalLengthCm + PetalWidthCm, data = train_data, method = \"class\")\n\n# Visualize the decision tree\nplot(iris_model)\ntext(iris_model, cex = 0.8)\n\n\n\n\n\nCode\n# Make predictions on the test set\npredictions &lt;- predict(iris_model, newdata = test_data, type = \"class\")\n\n# Evaluate the model\nconf_matrix &lt;- table(predictions, test_data$Species)\naccuracy &lt;- sum(diag(conf_matrix)) / sum(conf_matrix)\nprint(paste(\"Accuracy:\", round(accuracy, 2)))\n\n\n[1] \"Accuracy: 0.98\"\n\n\nCode\n# If you want to see the confusion matrix\nprint(\"Confusion Matrix:\")\n\n\n[1] \"Confusion Matrix:\"\n\n\nCode\nprint(conf_matrix)\n\n\n                 \npredictions       Iris-setosa Iris-versicolor Iris-virginica\n  Iris-setosa              14               0              0\n  Iris-versicolor           0              18              1\n  Iris-virginica            0               0             12"
  },
  {
    "objectID": "posts/d/index.html#how-to-read-the-data",
    "href": "posts/d/index.html#how-to-read-the-data",
    "title": "General R knowledge",
    "section": "",
    "text": "The first row in NFLX data set: On 09‐Aug-2002 (ANNDATS), analyst 6749 (ANALYS) at Estimator 1872 (ESTIMATOR) predicts that the EPS (MEASURE) for NETFLIX INC. (CNAME) with a ticker of NFLX (TICKER) with forecast period ending 30‐Sep-2002 (FPEDATS) is -$0.0086 (VALUE). This estimates was entered into the database on 12‐Aug-2002 (ACTDATS). On 17-Oct-2002 (ANNDATS_ACT), NETFLIX INC. announced an actual EPS of $7e-04 ($0.0007) (ACTUAL) for this quarter (FPI=6).\n\nhead(NFLX,n=1)\n\n  TICKER        CNAME  ACTDATS ESTIMATOR ANALYS FPI MEASURE   VALUE  FPEDATS\n1   NFLX NETFLIX INC. 20020812      1872   6749   6     EPS -0.0086 20020930\n   REVDATS  REVTIMS  ANNDATS  ANNTIMS ACTUAL ANNDATS_ACT ANNTIMS_ACT\n1 20021018 17:02:56 20020809 14:00:00 -7e-04    20021017    17:04:00"
  },
  {
    "objectID": "posts/d/index.html#your-turn",
    "href": "posts/d/index.html#your-turn",
    "title": "General R knowledge",
    "section": "",
    "text": "Task 1A: Calculate Missingness\n\n\n\nCheck to see the missing values in NFLX dataset and calculate the percent missing for each variable in NFLX and list your findings in R object called NFLX_missingness. NFLX_missingness is a dataframe with two columns: The first column, Variable, stores the variable names and the second column, Missingness shows the percent missing in percentage points with two decimal points."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-1a",
    "href": "posts/d/index.html#your-code-for-task-1a",
    "title": "General R knowledge",
    "section": "",
    "text": "library(tidyr)\nNFLX &lt;- read.csv(\"C:/Users/HP/Downloads/NFLX.csv\", header=TRUE)\nnames(NFLX)\n\n [1] \"TICKER\"      \"CNAME\"       \"ACTDATS\"     \"ESTIMATOR\"   \"ANALYS\"     \n [6] \"FPI\"         \"MEASURE\"     \"VALUE\"       \"FPEDATS\"     \"REVDATS\"    \n[11] \"REVTIMS\"     \"ANNDATS\"     \"ANNTIMS\"     \"ACTUAL\"      \"ANNDATS_ACT\"\n[16] \"ANNTIMS_ACT\"\n\n# Calculate the percent missing for each variable in NFLX\nNFLX_missingness &lt;- NFLX %&gt;%\n    summarise_all(~mean(is.na(.)) * 100)\n\n# Print the NFLX_missingness data frame\nprint(NFLX_missingness)\n\n  TICKER CNAME ACTDATS ESTIMATOR ANALYS FPI MEASURE VALUE FPEDATS REVDATS\n1      0     0       0         0      0   0       0     0       0       0\n  REVTIMS ANNDATS ANNTIMS   ACTUAL ANNDATS_ACT ANNTIMS_ACT\n1       0       0       0 4.115304    4.115304           0\n\n\n\n\n\n\n\n\nTask 1B: Data Manipulation\n\n\n\nConduct the following data manipulations on NFLX:\n\nDrop rows from the data set when a variable has a missing value\nDrop rows from the data set the quarterly forecasts (drop FPI=6)\nDeclare TICKER, CNAME, ESTIMATOR , ANALYS, FPI , and MEASURE variables as factor\nDeclare ACTDATS, FPEDATS , ANNDATS, REVDATS, ANNDATS_ACT as time variable.\nDrop ANNTIMS_ACT, ANNTIMS , and REVTIMS\nCreate a new column named YEAR that captures the year in FPEDATS\nName your reduced dataset as NFLX1\nPrint out data structure and the summary of NFLX1"
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-1b",
    "href": "posts/d/index.html#your-code-for-task-1b",
    "title": "General R knowledge",
    "section": "",
    "text": "# Copy NFLX to NFLX1 without assigning data types\nNFLX1 &lt;- NFLX\n\n# Drop rows from the data set when a variable has a missing value\nNFLX1 &lt;- NFLX1 %&gt;% na.omit()\n\n# Drop rows from the data set where FPI=6\nNFLX1 &lt;- NFLX1 %&gt;% filter(FPI != 6)\n\n# Drop ANNTIMS_ACT, ANNTIMS, and REVTIMS\nNFLX1 &lt;- NFLX1 %&gt;% select(-ANNTIMS_ACT, -ANNTIMS, -REVTIMS)\n\n# Create a new column named YEAR that is an exact copy of the data in FPEDATS\nNFLX1 &lt;- NFLX1 %&gt;% mutate(YEAR = FPEDATS)\n\n# Print out data structure and the summary of NFLX1\nstr(NFLX1)\n\n'data.frame':   2603 obs. of  14 variables:\n $ TICKER     : chr  \"NFLX\" \"NFLX\" \"NFLX\" \"NFLX\" ...\n $ CNAME      : chr  \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" ...\n $ ACTDATS    : int  20020805 20021202 20021202 20021202 20021205 20030106 20030115 20030116 20030121 20030314 ...\n $ ESTIMATOR  : int  183 2178 1872 220 2178 1872 2227 220 1872 481 ...\n $ ANALYS     : int  79868 80485 6749 57596 80485 6749 82629 57596 6749 81599 ...\n $ FPI        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ MEASURE    : chr  \"EPS\" \"EPS\" \"EPS\" \"EPS\" ...\n $ VALUE      : num  -0.025 -0.0321 -0.0207 -0.0179 -0.0286 -0.0136 -0.0164 -0.0071 0.0107 0.0129 ...\n $ FPEDATS    : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n $ REVDATS    : int  20021129 20021202 20021202 20021206 20021205 20030114 20030115 20030417 20030402 20030409 ...\n $ ANNDATS    : int  20020805 20021202 20021202 20021202 20021204 20030102 20030115 20030116 20030116 20030314 ...\n $ ACTUAL     : num  -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 0.0393 0.0393 0.0393 ...\n $ ANNDATS_ACT: int  20030115 20030115 20030115 20030115 20030115 20030115 20030115 20040121 20040121 20040121 ...\n $ YEAR       : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:217] 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 ...\n  ..- attr(*, \"names\")= chr [1:217] \"5057\" \"5058\" \"5059\" \"5060\" ...\n\nsummary(NFLX1)\n\n    TICKER             CNAME              ACTDATS           ESTIMATOR   \n Length:2603        Length:2603        Min.   :20020805   Min.   :  11  \n Class :character   Class :character   1st Qu.:20101021   1st Qu.: 192  \n Mode  :character   Mode  :character   Median :20141009   Median : 899  \n                                       Mean   :20136831   Mean   :1376  \n                                       3rd Qu.:20180122   3rd Qu.:2502  \n                                       Max.   :20210119   Max.   :4439  \n     ANALYS            FPI      MEASURE              VALUE       \n Min.   :  1047   Min.   :1   Length:2603        Min.   :-0.150  \n 1st Qu.: 71755   1st Qu.:1   Class :character   1st Qu.: 0.190  \n Median : 82010   Median :1   Mode  :character   Median : 0.430  \n Mean   : 89534   Mean   :1                      Mean   : 1.339  \n 3rd Qu.:114459   3rd Qu.:1                      3rd Qu.: 2.015  \n Max.   :194536   Max.   :1                      Max.   : 7.670  \n    FPEDATS            REVDATS            ANNDATS             ACTUAL      \n Min.   :20021231   Min.   :20021129   Min.   :20020805   Min.   :-0.005  \n 1st Qu.:20101231   1st Qu.:20110120   1st Qu.:20101021   1st Qu.: 0.250  \n Median :20141231   Median :20141013   Median :20141009   Median : 0.430  \n Mean   :20137082   Mean   :20137740   Mean   :20136830   Mean   : 1.384  \n 3rd Qu.:20181231   3rd Qu.:20180122   3rd Qu.:20180122   3rd Qu.: 2.680  \n Max.   :20201231   Max.   :20210119   Max.   :20210119   Max.   : 6.080  \n  ANNDATS_ACT            YEAR         \n Min.   :20030115   Min.   :20021231  \n 1st Qu.:20110126   1st Qu.:20101231  \n Median :20150120   Median :20141231  \n Mean   :20145973   Mean   :20137082  \n 3rd Qu.:20190117   3rd Qu.:20181231  \n Max.   :20210119   Max.   :20201231  \n\n\n\n\n\n\n\n\nTask 2: Calculate Number of Analysts and Brokerage Houses\n\n\n\n\nCalculate the total number of unique analysts in NFLX1 dataset that provide forecasts each year and name your R object as NumberAnalyst\nCalculate the total number of unique brokerage houses (ESTIMATOR) in NFLX1 dataset that provide forecasts each year and name your R object as NumberBrokerage\nNeed Written Response in this callout: In which year(s) we have the highest number of unique analysts providing forecasts for NFLX ticker? In which year(s), we have the highest number of unique brokerage houses providing forecasts for the NFLX ticker.\n\n2011 is the year number of distinct analysts providing forecasts for netflix."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-2",
    "href": "posts/d/index.html#your-code-for-task-2",
    "title": "General R knowledge",
    "section": "",
    "text": "# Calculate the total number of unique analysts providing forecasts each year\nNumberAnalyst &lt;- NFLX1 %&gt;%\n  group_by(YEAR) %&gt;%\n  distinct(ESTIMATOR) %&gt;%\n  summarise(NumAnalysts = n_distinct(ESTIMATOR))\n\n# Print the NumberAnalyst object\nprint(NumberAnalyst)\n\n# A tibble: 19 × 2\n       YEAR NumAnalysts\n      &lt;int&gt;       &lt;int&gt;\n 1 20021231           5\n 2 20031231           8\n 3 20041231          18\n 4 20051231          17\n 5 20061231          19\n 6 20071231          18\n 7 20081231          21\n 8 20091231          32\n 9 20101231          38\n10 20111231          35\n11 20121231          36\n12 20131231          43\n13 20141231          40\n14 20151231          46\n15 20161231          45\n16 20171231          49\n17 20181231          54\n18 20191231          43\n19 20201231          44\n\n# Calculate the total number of unique brokerage houses providing forecasts each year\nNumberBrokerage &lt;- NFLX1 %&gt;%\n  group_by(YEAR) %&gt;%\n  distinct(ESTIMATOR) %&gt;%\n  summarise(NumBrokerage = n_distinct(ESTIMATOR))\n\n# Print the NumberBrokerage object\nprint(NumberBrokerage)\n\n# A tibble: 19 × 2\n       YEAR NumBrokerage\n      &lt;int&gt;        &lt;int&gt;\n 1 20021231            5\n 2 20031231            8\n 3 20041231           18\n 4 20051231           17\n 5 20061231           19\n 6 20071231           18\n 7 20081231           21\n 8 20091231           32\n 9 20101231           38\n10 20111231           35\n11 20121231           36\n12 20131231           43\n13 20141231           40\n14 20151231           46\n15 20161231           45\n16 20171231           49\n17 20181231           54\n18 20191231           43\n19 20201231           44\n\n\n\n\n\n\n\n\nTask 3: Get the most recent forecast in each year\n\n\n\n\nIt is quite possible that an analyst makes multiple forecasts throughout the year for the same fiscal period. Remove observations from NFLX1 if an analyst has multiple predictions for the same year and keep the last one (the most recent forecast for each year). Name your new dataset as NFLX2. This step is crucial for successful execution of the following tasks. Print the dimension of NFLX2.\nCheck your work: If your NFLX2 dataset has 641 rows and 14 columns, then you are on the right track. If not, please seek help!"
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-3",
    "href": "posts/d/index.html#your-code-for-task-3",
    "title": "General R knowledge",
    "section": "",
    "text": "# Task 3: Get the most recent forecast in each year\n\n# Get the most recent forecast for each analyst in each year\nNFLX2 &lt;- NFLX1 %&gt;%\n    # Group by analyst and year\n    group_by(ANALYS, YEAR) %&gt;%\n    # Filter to keep the most recent forecast for each analyst in each year\n    filter(\n        REVDATS == max(REVDATS)\n    ) %&gt;%\n    # Ungroup the data frame\n    ungroup()\n\n# Print the dimension of NFLX2\ndim(NFLX2)\n\n[1] 641  14\n\n# Check your work: If your NFLX2 dataset has 641 rows and 14 columns, then you are on the right track. If not, please seek help!\n\n\n\n\n\n\n\n\n\nTask 4: Calculate past accuracy\n\n\n\n\nCreate a copy of NFLX2 and call it NFLX3\nFor every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy. In the calculation of forecast performance, you can use the VALUE-ACTUAL as the forecast accuracy measure.\nFor each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy\nAs an example, consider the year 2006, where analyst 1047, employed at brokerage house 464, provided an estimated end-of-period EPS of 0.0929 (VALUE). However, the actual EPS for that year turned out to be 0.1014 (ACTUAL), resulting in a forecasting error of -0.0085. Consequently, in the subsequent year, 2007, the past_accuracy metric for analyst 1047 would reflect this error by taking the value of -0.0085 (VALUE-ACTUAL).\nThis action will create some missing values and this is perfectly fine.\nIf your code produces 144 NAs, then you are on the right track.\nNote that we are creating copies of the original dataset at each step to facilitate error detection in case any mistakes occur during the process."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-4",
    "href": "posts/d/index.html#your-code-for-task-4",
    "title": "General R knowledge",
    "section": "",
    "text": "# Create a copy of NFLX2 and call it NFLX3\nNFLX3 &lt;- NFLX2\n\n# Task 4: Calculate past accuracy\n\n# For every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy.\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ANALYS) %&gt;%\n  mutate(accuracy = VALUE - ACTUAL)\n\n# For each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  arrange(YEAR) %&gt;%\n  mutate(past_accuracy = lag(accuracy))\n\n# Check if the code produces 144 NAs\nsum(is.na(NFLX3$past_accuracy))\n\n[1] 144\n\n\n\n\n\n\n\n\n\n\nTask 5: Forecast Horizon\n\n\n\n\nThe longer the forecast horizon, the higher the uncertainty associated with EPS forecasts. To control for this fact, create a new column in NFLX3 called horizon that captures the forecast horizon (ANNDATS_ACT- ANNDATS) for each analyst.\nWe anticipate observing a negative correlation between accuracy and horizon. Typically, as the forecast horizon increases, the accuracy tends to decrease, and vice versa. However, in our dataset, there is an exception where we find a positive correlation between accuracy and horizon for one specific year. Write an R code to identify and determine which year exhibits this positive correlation.\nNeed Written Response in this callout: Enter the year in here.\n2011 2012 2103 2015 and 2018 with 2018 having a forecast value of 0.24."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-5",
    "href": "posts/d/index.html#your-code-for-task-5",
    "title": "General R knowledge",
    "section": "",
    "text": "# Task 5: Forecast Horizon\n# Calculate past accuracy and forecast horizon\nNFLX3 &lt;- NFLX3 %&gt;% group_by(ANALYS) %&gt;% arrange(YEAR) %&gt;% mutate(past_accuracy = lag(accuracy, default = NA))\nNFLX3 &lt;- NFLX3 %&gt;% mutate(horizon = as.numeric(difftime(ANNDATS_ACT, ANNDATS, units = \"days\")))\n\n# Calculate correlation and find positive correlation year\ncorrelation_by_year &lt;- NFLX3 %&gt;% group_by(YEAR) %&gt;% summarise(correlation = cor(accuracy, horizon, use = \"complete.obs\"))\npositive_corr_year &lt;- correlation_by_year %&gt;% filter(correlation &gt; 0)\n\n# Print positive correlation year with correlation values\npositive_corr_year %&gt;% mutate(correlation = round(correlation, 2)) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = \"striped\")\n\n\n\n\nYEAR\ncorrelation\n\n\n\n\n20111231\n0.23\n\n\n20121231\n0.07\n\n\n20131231\n0.02\n\n\n20151231\n0.06\n\n\n20181231\n0.24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 6: Experience\n\n\n\n\nWe assume that if an analyst is monitoring a company for a long period of time, he/she is expected to make more informed predictions. Create a new column in NFLX3 called experience that counts the cumulative number of years the analyst monitor (have predictions) the company. Print the summary of experience column.\nHint: Try to use cumsum() function in R.\nNeed Written Response in this callout: Which analyst (s) has the highest number of experience in NFLX3 dataset and for how long do they monitor the NFLX ticker?\n\nAnalyst 72088 and 77748 have 17 years of hardcore experience"
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-6",
    "href": "posts/d/index.html#your-code-for-task-6",
    "title": "General R knowledge",
    "section": "",
    "text": "# Task 6: Experience\n\n# Calculate the cumulative experience for each analyst\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  mutate(experience = cumsum(!duplicated(YEAR)))\n\n# Find the analyst(s) with the highest experience\nmax_experience &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  summarise(experience = max(experience)) %&gt;%\n  filter(experience == max(experience))\n\n# Print the summary of the experience column\nprint(summary(NFLX3$experience))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.192   6.000  17.000 \n\n# Find the analyst(s) with the highest experience\nmax_experience &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  summarise(experience = max(experience)) %&gt;%\n  filter(experience == max(experience))\n\n# Print the summary of the experience column\nprint(summary(NFLX3$experience))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.192   6.000  17.000 \n\n# Print the analyst(s) with the highest experience\nprint(max_experience)\n\n# A tibble: 2 × 2\n  ANALYS experience\n   &lt;int&gt;      &lt;int&gt;\n1  72088         17\n2  77748         17\n\n\n\n\n\n\n\n\n\n\nTask 7: Size\n\n\n\n\nIf a brokerage house has multiple analysts providing predictions for the same company, it may indicate a greater allocation of resources for company analysis. To capture this, create a new column in the NFLX3 dataset called size that calculates the total count of unique analysts employed per year by each brokerage house (ESTIMATOR)\nNeed Written Response in this callout: Print the frequencies for size variable. What does this frequency table reveal about the distribution of the number of analysts hired by brokerage houses in this dataset?\n\nFrom the table we can conclude that as the number of analysts increase the frequency of hiring reduces exponentially. This indicates that a brokerage preferred hiring one analyst per every season."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-7",
    "href": "posts/d/index.html#your-code-for-task-7",
    "title": "General R knowledge",
    "section": "",
    "text": "# Task 7: Size\n\n# Calculate the total count of unique analysts employed per year by each brokerage house\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ESTIMATOR) %&gt;%\n  mutate(size = n_distinct(ANALYS))\n\n# Print the frequencies for the size variable\nsize_freq &lt;- table(NFLX3$size)\nprint(size_freq)\n\n\n  1   2   3 \n560  72   9 \n\n# Create a frequency table for better visualization\nsize_table &lt;- as.data.frame(size_freq)\ncolnames(size_table) &lt;- c(\"Number of Analysts\", \"Frequency\")\n\n# Sort the table by frequency in descending order\nsize_table &lt;- size_table[order(-size_table$Frequency), ]\n\n# Print the sorted frequency table\nprint(size_table)\n\n  Number of Analysts Frequency\n1                  1       560\n2                  2        72\n3                  3         9\n\n# Summary statistics for size variable\nsummary(NFLX3$size)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.00    1.00    1.14    1.00    3.00 \n\n\n\n\n\n\n\n\n\n\nTask 8: Prediction 1\n\n\n\n\nIn the year 2020, NETFLIX reported an actual earnings per share (EPS) of $6.08. To predict this EPS value based on historical data, we will employ a linear regression model using the dataset NFLX3 up until the year 2019. In this model, the target variable will be ACTUAL and the predictor variables will include VALUE and past_accuracy. C.all your model as model1.\nNeed Written Response in this callout: Using the linear regression model ‘model1,’ which has been trained on historical data up to the year 2019, what is the forecasted EPS (Earnings Per Share) for the year 2020? Please provide a brief explanation of the method you employed to make this prediction. If you encountered any challenges or were unable to make the calculation, briefly describe the specific issues you encountered.\n\nUsing the linear regression model 'model1,' trained on historical data up to the year 2019, we can forecast the EPS (Earnings Per Share) for the year 2020. The method employed involves calculating the R-squared value of the model, which measures the goodness of fit. If the R-squared value is high (greater than 0.5 in this case), we proceed with forecasting.\nIn this example, the code calculates the mean of the 'past_accuracy' variable. If the R-squared value is satisfactory, we generate a forecast for the future period. We create a new data frame for the future period with values for the independent variables (VALUE and past_accuracy). The code then uses the 'predict' function to estimate the EPS for the future period.\nIf the R-squared value is low, a warning message is provided, indicating that the model may not accurately predict future EPS.\nThe method allows us to make forecasts if the model's fit is deemed appropriate. However, we encountered issues with data configuration or model training, those specific challenges should be addressed to ensure the accuracy of the model"
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-8",
    "href": "posts/d/index.html#your-code-for-task-8",
    "title": "General R knowledge",
    "section": "",
    "text": "# Calculate the mean of past_accuracy\nmean_past_accuracy &lt;- mean(NFLX3$past_accuracy, na.rm = TRUE)\n\n# Convert the 'YEAR' column to the desired format\nNFLX3$YEAR &lt;- as.POSIXct(NFLX3$YEAR, format = \"%Y-%m-%d %H:%M:%S\")\n\n# Create the linear regression model using historical data up to the year 2019\nmodel1 &lt;- lm(ACTUAL ~ VALUE + past_accuracy, data = NFLX3)\n\n# Get the R-squared value of the model\nr_squared &lt;- summary(model1)$r.squared\n\n# If the R-squared value is high, then we can use the model to generate a forecast\nif (r_squared &gt; 0.5) {\n  # Create a new data frame for the future period with the values of the independent variables\n  new_data_future &lt;- data.frame(\n    VALUE = 6.08, # Replace this with the actual value of VALUE for the future period\n    past_accuracy = mean_past_accuracy\n  )\n\n  # Predict the EPS for the future period\n  predicted_eps_future &lt;- predict(model1, newdata = new_data_future)\n\n  # Print the forecasted EPS for the future period\n  cat(\"Forecasted EPS for future period: $\", round(predicted_eps_future, 2))\n} else {\n  # Print a warning message\n  cat(\"The R-squared value is low, so the model may not be able to accurately predict future values of the dependent variable.\")\n}\n\nForecasted EPS for future period: $ 6.3\n\n# Print the mean of past_accuracy\ncat(\"Mean past_accuracy: \", round(mean_past_accuracy, 2))\n\nMean past_accuracy:  -0.09\n\n\n\n\n\n\n\n\n\n\nTask 9: Prediction 2\n\n\n\n\nAs an alternative approach, instead of modeling the ‘ACTUAL’ value, we can obtain the mean and median forecasts for the year 2020 as our best estimates of the EPS value for that year.\nNeed Written Response in this callout: Please calculate these forecasts and then compare them with the results from the previous task. Finally, provide your insights and comments based on your findings.\n\nIn this alternative approach, we opted to calculate the mean and median forecasts for the year 2020 as our best estimates of the EPS value. The mean forecast for 2020 is approximately $1.24, while the median forecast is significantly lower, at approximately $0.41. When comparing these results with the linear regression model from the previous task, it's evident that the model-based forecast might provide a more detailed and potentially accurate prediction. However, these two approaches have their unique merits and drawbacks. The model-driven forecast takes into account historical relationships and variables like 'past_accuracy,' but it heavily relies on the quality of the model fit, as indicated by the R-squared value. On the other hand, the mean and median forecasts provide simple summary statistics but might lack the predictive power of a well-fitted model. The choice between these methods should be influenced by the data quality and the context of the analysis."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-9",
    "href": "posts/d/index.html#your-code-for-task-9",
    "title": "General R knowledge",
    "section": "",
    "text": "# Calculate the mean forecast\nmean_forecast &lt;- mean(NFLX3$VALUE, na.rm = TRUE)\n\n# Calculate the median forecast\nmedian_forecast &lt;- median(NFLX3$VALUE, na.rm = TRUE)\n\n# Print the mean and median forecasts\ncat(\"Mean forecast for 2020: $\", round(mean_forecast, 2))\n\nMean forecast for 2020: $ 1.24\n\ncat(\"Median forecast for 2020: $\", round(median_forecast, 2))\n\nMedian forecast for 2020: $ 0.41\n\n\n\n\n\n\n\n\n\n\nTask 10: Averages\n\n\n\n\nGenerate a new dataset named NFLX4 by aggregating data from NFLX3 Include the variables size, experience, horizon, accuracy, past_accuracy, and ACTUAL in NFLX4. When calculating the yearly averages for these variables, ignore any missing values (NAs). Present a summary of the NFLX4 dataset.\nNeed Written Response in this callout: Subsequently, employ correlation analysis or exploratory data analysis to get insights into the relationships between these variables and ‘ACTUAL,’ if such relationships exist.\n\nAfter conducting correlation analysis and exploratory data analysis on the dataset NFLX4, several interesting insights into the relationships between variables and 'ACTUAL' have emerged\nThe correlation matrix reveals that 'ACTUAL' is positively correlated with 'size' (0.18) and 'experience' (0.69), indicating that, on average, larger groups of analysts and analysts with more experience tend to provide more accurate forecasts for the 'ACTUAL' earnings per share.\nConversely, 'ACTUAL' is negatively correlated with 'horizon' (-0.63) and 'past_accuracy' (-0.80). This suggests that analysts with a longer forecasting horizon and those who have made more accurate predictions in the past tend to have less accurate forecasts for 'ACTUAL' earnings per share.\nIn the exploratory data analysis, scatter plots further highlight these relationships. For instance, the scatter plot of 'ACTUAL' against 'size' shows a general trend of improved accuracy as the number of analysts increases.\nThese findings provide valuable insights for understanding the factors that influence the accuracy of earnings per share forecasts ('ACTUAL'). Larger analyst groups and more experienced analysts tend to provide more accurate forecasts, while longer forecasting horizons and a history of past accuracy can lead to less accurate predictions. These relationships can be crucial for analysts, investors, and decision-makers in the financial domain.\n\n\n\n# Aggregate data and calculate yearly averages, ignoring missing values (NAs)\nNFLX4 &lt;- NFLX3 %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(\n    size = mean(size, na.rm = TRUE),\n    experience = mean(experience, na.rm = TRUE),\n    horizon = mean(horizon, na.rm = TRUE),\n    accuracy = mean(accuracy, na.rm = TRUE),\n    past_accuracy = mean(past_accuracy, na.rm = TRUE),\n    ACTUAL = mean(ACTUAL, na.rm = TRUE)\n  )\n\n# Present a summary of the NFLX4 dataset\nsummary(NFLX4)\n\n      YEAR                          size         experience   \n Min.   :1970-08-20 20:27:11   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1970-08-21 08:57:11   1st Qu.:1.074   1st Qu.:2.664  \n Median :1970-08-21 21:27:11   Median :1.105   Median :3.400  \n Mean   :1970-08-21 21:27:11   Mean   :1.132   Mean   :3.611  \n 3rd Qu.:1970-08-22 09:57:11   3rd Qu.:1.202   3rd Qu.:4.869  \n Max.   :1970-08-22 22:27:11   Max.   :1.300   Max.   :6.061  \n                                                              \n    horizon           accuracy         past_accuracy           ACTUAL       \n Min.   :0.06284   Min.   :-0.822085   Min.   :-0.798219   Min.   :-0.0050  \n 1st Qu.:0.08547   1st Qu.:-0.019087   1st Qu.:-0.028736   1st Qu.: 0.0914  \n Median :0.09289   Median :-0.015035   Median :-0.013423   Median : 0.2643  \n Mean   :0.09004   Mean   :-0.048310   Mean   :-0.060652   Mean   : 0.9248  \n 3rd Qu.:0.09512   3rd Qu.:-0.005415   3rd Qu.:-0.009260   3rd Qu.: 0.5678  \n Max.   :0.10656   Max.   : 0.121449   Max.   :-0.001547   Max.   : 6.0800  \n                                       NA's   :1                            \n\n# Correlation analysis\ncorrelation_matrix &lt;- cor(NFLX4[, c(\"size\", \"experience\", \"horizon\", \"accuracy\", \"past_accuracy\", \"ACTUAL\")], use = \"complete.obs\")\n\n# Print correlation matrix\nprint(correlation_matrix)\n\n                     size  experience    horizon    accuracy past_accuracy\nsize           1.00000000  0.07451284 -0.1317823 -0.04537307    -0.1810330\nexperience     0.07451284  1.00000000 -0.4844637 -0.25882136    -0.4620906\nhorizon       -0.13178225 -0.48446371  1.0000000  0.22264895     0.4979377\naccuracy      -0.04537307 -0.25882136  0.2226489  1.00000000    -0.1604379\npast_accuracy -0.18103301 -0.46209061  0.4979377 -0.16043792     1.0000000\nACTUAL         0.18223220  0.68707354 -0.6346966 -0.31928984    -0.7958850\n                  ACTUAL\nsize           0.1822322\nexperience     0.6870735\nhorizon       -0.6346966\naccuracy      -0.3192898\npast_accuracy -0.7958850\nACTUAL         1.0000000\n\n# Exploratory data analysis\n# Create scatter plots to explore relationships\n\n# Scatter plot of ACTUAL vs. size\nggplot(NFLX4, aes(x = size, y = ACTUAL)) +\n  geom_point(color = \"blue\", shape = 1) +\n  ggtitle(\"ACTUAL vs. size\") +\n  xlab(\"size\") +\n  ylab(\"ACTUAL\")\n\n\n\n# Scatter plot of ACTUAL vs. experience\nggplot(NFLX4, aes(x = experience, y = ACTUAL)) +\n  geom_point(color = \"green\", shape = 2) +\n  ggtitle(\"ACTUAL vs. experience\") +\n  xlab(\"experience\") +\n  ylab(\"ACTUAL\")\n\n\n\n# Scatter plot of ACTUAL vs. horizon\nggplot(NFLX4, aes(x = horizon, y = ACTUAL)) +\n  geom_point(color = \"red\", shape = 3) +\n  ggtitle(\"ACTUAL vs. horizon\") +\n  xlab(\"horizon\") +\n  ylab(\"ACTUAL\")\n\n\n\n# Scatter plot of ACTUAL vs. accuracy\nggplot(NFLX4, aes(x = accuracy, y = ACTUAL)) +\n  geom_point(color = \"orange\", shape = 4) +\n  ggtitle(\"ACTUAL vs. accuracy\") +\n  xlab(\"accuracy\") +\n  ylab(\"ACTUAL\")\n\n\n\n# Scatter plot of ACTUAL vs. past_accuracy\nggplot(NFLX4, aes(x = past_accuracy, y = ACTUAL)) +\n  geom_point(color = \"purple\", shape = 5) +\n  ggtitle(\"ACTUAL vs. past_accuracy\") +\n  xlab(\"past_accuracy\") +\n  ylab(\"ACTUAL\")\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n# Save NFLX4 to a CSV file if needed\n# write.csv(NFLX4, \"NFLX4.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/post with code/index.html",
    "href": "posts/post with code/index.html",
    "title": "Unveiling the Marvels of Data Science: A Journey into the Heart of Information",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(reshape2)\n\nWarning: package 'reshape2' was built under R version 4.3.2\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# Load the dataset\nHousingData &lt;- read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\")\n\n# Display the structure of the dataset\nstr(HousingData)\n\n'data.frame':   506 obs. of  14 variables:\n $ CRIM   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ ZN     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ INDUS  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ CHAS   : int  0 0 0 0 0 0 NA 0 0 NA ...\n $ NOX    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ RM     : num  6.58 6.42 7.18 7 7.15 ...\n $ AGE    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ DIS    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ RAD    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ TAX    : int  296 242 242 222 222 222 311 311 311 311 ...\n $ PTRATIO: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ B      : num  397 397 393 395 397 ...\n $ LSTAT  : num  4.98 9.14 4.03 2.94 NA ...\n $ MEDV   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n\n# Display summary statistics\nsummary(HousingData)\n\n      CRIM                ZN             INDUS            CHAS        \n Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n 1st Qu.: 0.08190   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n Median : 0.25372   Median :  0.00   Median : 9.69   Median :0.00000  \n Mean   : 3.61187   Mean   : 11.21   Mean   :11.08   Mean   :0.06996  \n 3rd Qu.: 3.56026   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n NA's   :20         NA's   :20       NA's   :20      NA's   :20       \n      NOX               RM             AGE              DIS        \n Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n 1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.17   1st Qu.: 2.100  \n Median :0.5380   Median :6.208   Median : 76.80   Median : 3.207  \n Mean   :0.5547   Mean   :6.285   Mean   : 68.52   Mean   : 3.795  \n 3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 93.97   3rd Qu.: 5.188  \n Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n                                  NA's   :20                       \n      RAD              TAX           PTRATIO            B         \n Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32  \n 1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38  \n Median : 5.000   Median :330.0   Median :19.05   Median :391.44  \n Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67  \n 3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23  \n Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90  \n                                                                  \n     LSTAT             MEDV      \n Min.   : 1.730   Min.   : 5.00  \n 1st Qu.: 7.125   1st Qu.:17.02  \n Median :11.430   Median :21.20  \n Mean   :12.715   Mean   :22.53  \n 3rd Qu.:16.955   3rd Qu.:25.00  \n Max.   :37.970   Max.   :50.00  \n NA's   :20                      \n\n# Check for missing values\nsum(is.na(HousingData))\n\n[1] 120\n\n# Explore the distribution of the target variable (MEDV)\nggplot(HousingData, aes(x = MEDV)) +\n  geom_histogram(fill = \"blue\", bins = 30) +\n  labs(title = \"Distribution of Median Home Values\",\n       x = \"Median Home Value ($1000's)\",\n       y = \"Frequency\")\n\n\n\n# Explore relationships between variables using scatter plots\npairs(HousingData[, c(\"CRIM\", \"RM\", \"AGE\", \"DIS\", \"TAX\", \"LSTAT\", \"MEDV\")])\n\n\n\n# Correlation matrix to identify relationships between variables\ncor_matrix &lt;- cor(HousingData)\n\n# Display a heatmap of the correlation matrix\nggplot(data = melt(cor_matrix), aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Correlation Heatmap\",\n       x = \"Variables\",\n       y = \"Variables\")\n\n\n\n# Boxplots to visualize the distribution of variables\npar(mfrow = c(4, 4), mar = c(2, 2, 2, 2))  # Adjust margin parameters\nfor (i in 1:14) {\n  boxplot(HousingData[, i], main = colnames(HousingData)[i], col = \"lightblue\")\n}\n\n# Explore relationships between selected variables\nggplot(HousingData, aes(x = RM, y = MEDV)) +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"Scatter Plot of RM vs. MEDV\",\n       x = \"Average Number of Rooms\",\n       y = \"Median Home Value ($1000's)\")\n\n# Conduct more exploratory data analysis based on your specific questions and hypotheses\n\n# Example: Analyzing the impact of crime rate on median home values\nggplot(HousingData, aes(x = CRIM, y = MEDV)) +\n  geom_point(alpha = 0.6, color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(title = \"Impact of Crime Rate on Median Home Values\",\n       x = \"Per Capita Crime Rate\",\n       y = \"Median Home Value ($1000's)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 20 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 20 rows containing missing values (`geom_point()`).\n\n\n\n\n\nExploratory Data Analysis (EDA) is a crucial step in understanding and gaining insights from a dataset. In the case of the Boston Housing Dataset, the summary statistics reveal the variability and distribution of key features such as crime rate (CRIM), proportion of residential land (ZN), and non-retail business acres (INDUS). The dataset contains information about factors like air quality (NOX), average number of rooms (RM), and socio-economic status (LSTAT), which may influence the median value of owner-occupied homes (MEDV). Handling missing values is essential, and in this dataset, 20 rows have missing entries in various columns. Through graphical representations such as histograms and scatter plots, relationships between variables can be observed, aiding in the identification of patterns and potential outliers. The dataset provides a comprehensive view of diverse factors influencing housing values, setting the stage for further in-depth analysis and modeling.\nThe correlation matrix provides insights into the relationships between various features in the dataset. Examining the correlation matrix for the Boston Housing Dataset, it is evident that certain pairs of variables exhibit notable correlations. For instance, the variable \"RM,\" representing the average number of rooms per dwelling, displays a positive correlation with the target variable \"MEDV,\" indicating a higher median home value with an increased number of rooms. Conversely, the \"LSTAT\" variable, representing the percentage of the lower status of the population, demonstrates a negative correlation with \"MEDV,\" suggesting that areas with a higher percentage of lower-status residents tend to have lower median home values. These insights from the correlation matrix lay the foundation for further exploration and model building, guiding the selection of relevant features for predictive analysis in the Boston Housing Dataset.\n\n\n# Install and load the required packages\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.2\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(ggplot2)\n\n# Load the dataset\ndata &lt;- read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\")\n\n# Remove rows with missing values\ndata &lt;- na.omit(data)\n\n# Separate features (X) and target variable (y)\nX &lt;- data[, colnames(data) != \"MEDV\"]\ny &lt;- data$MEDV\n\n# Split the data into training and testing sets\nset.seed(42)\nindices &lt;- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data &lt;- data[indices, ]\ntest_data &lt;- data[-indices, ]\n\n# Initialize the Random Forest Regressor\nrf_model &lt;- randomForest(MEDV ~ ., data = train_data, ntree = 100)\n\n# Make predictions on the test set\npredictions &lt;- predict(rf_model, newdata = test_data)\n\n# Evaluate the model\nmse &lt;- mean((predictions - test_data$MEDV)^2)\nr_squared &lt;- 1 - mse / var(test_data$MEDV)\n\nprint(paste(\"Mean Squared Error:\", mse))\n\n[1] \"Mean Squared Error: 10.243041067851\"\n\nprint(paste(\"R-squared:\", r_squared))\n\n[1] \"R-squared: 0.897607585182528\"\n\n# Visualization: Scatter plot of actual vs. predicted values\nggplot(data = test_data, aes(x = MEDV, y = predictions)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n\n\n\nThe machine learning model, utilizing a Random Forest Regressor with 100 trees, demonstrated promising performance on the Boston Housing Dataset. The evaluation metrics indicate a mean squared error (MSE) of 10.24 and an R-squared value of 0.90 on the test set. The MSE measures the average squared difference between the predicted and actual median values of owner-occupied homes, providing a quantitative assessment of the model’s accuracy. In this context, the relatively low MSE signifies that the model’s predictions are, on average, close to the true values. Additionally, the high R-squared value of 0.90 suggests that the majority of the variance in the target variable is explained by the model, indicating a strong predictive capability. These results imply that the Random Forest Regressor has effectively captured the complex relationships within the dataset, providing a reliable framework for predicting median home values based on the given features.\n\n\n# Install and load the required packages\nlibrary(pdp)\n\nWarning: package 'pdp' was built under R version 4.3.2\n\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.3.2\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(ggplot2)\n\n# Load the dataset\ndata &lt;- read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\")\n\n# Remove rows with missing values\ndata &lt;- na.omit(data)\n\n# Separate features (X) and target variable (y)\nX &lt;- data[, colnames(data) != \"MEDV\"]\ny &lt;- data$MEDV\n\n# Split the data into training and testing sets\nset.seed(42)\nindices &lt;- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data &lt;- data[indices, ]\ntest_data &lt;- data[-indices, ]\n\n# Initialize the XGBoost Regressor\nxgb_model &lt;- xgboost(data = as.matrix(train_data[, colnames(train_data) != \"MEDV\"]), label = train_data$MEDV, nrounds = 100, objective = \"reg:squarederror\")\n\n[1] train-rmse:16.864020 \n[2] train-rmse:12.196387 \n[3] train-rmse:8.915546 \n[4] train-rmse:6.600957 \n[5] train-rmse:4.953999 \n[6] train-rmse:3.785458 \n[7] train-rmse:2.948327 \n[8] train-rmse:2.351052 \n[9] train-rmse:1.895368 \n[10]    train-rmse:1.599467 \n[11]    train-rmse:1.400130 \n[12]    train-rmse:1.196252 \n[13]    train-rmse:1.092092 \n[14]    train-rmse:1.019197 \n[15]    train-rmse:0.941965 \n[16]    train-rmse:0.891416 \n[17]    train-rmse:0.844929 \n[18]    train-rmse:0.809411 \n[19]    train-rmse:0.775116 \n[20]    train-rmse:0.702806 \n[21]    train-rmse:0.663042 \n[22]    train-rmse:0.634505 \n[23]    train-rmse:0.574013 \n[24]    train-rmse:0.557150 \n[25]    train-rmse:0.512364 \n[26]    train-rmse:0.472643 \n[27]    train-rmse:0.449644 \n[28]    train-rmse:0.420094 \n[29]    train-rmse:0.407141 \n[30]    train-rmse:0.377183 \n[31]    train-rmse:0.347750 \n[32]    train-rmse:0.324683 \n[33]    train-rmse:0.311104 \n[34]    train-rmse:0.288693 \n[35]    train-rmse:0.267527 \n[36]    train-rmse:0.241400 \n[37]    train-rmse:0.223405 \n[38]    train-rmse:0.216449 \n[39]    train-rmse:0.200252 \n[40]    train-rmse:0.193583 \n[41]    train-rmse:0.181290 \n[42]    train-rmse:0.165571 \n[43]    train-rmse:0.159072 \n[44]    train-rmse:0.150027 \n[45]    train-rmse:0.140853 \n[46]    train-rmse:0.126037 \n[47]    train-rmse:0.123955 \n[48]    train-rmse:0.119162 \n[49]    train-rmse:0.114289 \n[50]    train-rmse:0.104554 \n[51]    train-rmse:0.102517 \n[52]    train-rmse:0.098968 \n[53]    train-rmse:0.095341 \n[54]    train-rmse:0.087070 \n[55]    train-rmse:0.083275 \n[56]    train-rmse:0.078488 \n[57]    train-rmse:0.075294 \n[58]    train-rmse:0.072737 \n[59]    train-rmse:0.069200 \n[60]    train-rmse:0.067622 \n[61]    train-rmse:0.061438 \n[62]    train-rmse:0.059305 \n[63]    train-rmse:0.057915 \n[64]    train-rmse:0.055674 \n[65]    train-rmse:0.051587 \n[66]    train-rmse:0.048178 \n[67]    train-rmse:0.046609 \n[68]    train-rmse:0.045095 \n[69]    train-rmse:0.042064 \n[70]    train-rmse:0.040559 \n[71]    train-rmse:0.038290 \n[72]    train-rmse:0.036909 \n[73]    train-rmse:0.034672 \n[74]    train-rmse:0.032852 \n[75]    train-rmse:0.031132 \n[76]    train-rmse:0.029724 \n[77]    train-rmse:0.027688 \n[78]    train-rmse:0.026797 \n[79]    train-rmse:0.025369 \n[80]    train-rmse:0.024092 \n[81]    train-rmse:0.023262 \n[82]    train-rmse:0.022374 \n[83]    train-rmse:0.021797 \n[84]    train-rmse:0.021347 \n[85]    train-rmse:0.019702 \n[86]    train-rmse:0.019364 \n[87]    train-rmse:0.017645 \n[88]    train-rmse:0.016802 \n[89]    train-rmse:0.016005 \n[90]    train-rmse:0.015056 \n[91]    train-rmse:0.014366 \n[92]    train-rmse:0.012951 \n[93]    train-rmse:0.012659 \n[94]    train-rmse:0.012012 \n[95]    train-rmse:0.011531 \n[96]    train-rmse:0.011218 \n[97]    train-rmse:0.010646 \n[98]    train-rmse:0.009803 \n[99]    train-rmse:0.008940 \n[100]   train-rmse:0.008829 \n\n# Make predictions on the test set\npredictions &lt;- predict(xgb_model, as.matrix(test_data[, colnames(test_data) != \"MEDV\"]))\n\n# Evaluate the model\nmse &lt;- mean((predictions - test_data$MEDV)^2)\nr_squared &lt;- 1 - mse / var(test_data$MEDV)\n\nprint(paste(\"Mean Squared Error:\", mse))\n\n[1] \"Mean Squared Error: 9.55062801978429\"\n\nprint(paste(\"R-squared:\", r_squared))\n\n[1] \"R-squared: 0.9045291472043\"\n\n# Visualization 1: Scatter plot of actual vs. predicted values\nggplot(data = test_data, aes(x = MEDV, y = predictions)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n\n\n# Visualization 2: Feature Importance Plot\nfeature_importance &lt;- xgb.importance(model = xgb_model)\nxgb.plot.importance(importance_matrix = feature_importance)\n\n\n\n\nThe XGBoost model demonstrates a notable reduction in training root mean squared error (train-rmse) throughout the training process, starting from 16.86 and progressively decreasing to 0.0088. The mean squared error on the test set is 9.55, indicating strong predictive performance. The high R-squared value of 0.9045 further confirms the model's ability to explain the variance in the target variable, suggesting that the XGBoost model effectively captures the underlying patterns in the housing data, making it a robust choice for regression tasks.\n\nlibrary(xgboost)\nlibrary(randomForest)\nlibrary(ggplot2)\n\n# Load and preprocess the data\ndata &lt;- na.omit(read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\"))\nset.seed(42)\nindices &lt;- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data &lt;- data[indices, ]\ntest_data &lt;- data[-indices, ]\n\n# Model A: XGBoost\nxgb_model &lt;- xgboost(data = as.matrix(train_data[, colnames(train_data) != \"MEDV\"]), label = train_data$MEDV, nrounds = 100, objective = \"reg:squarederror\")\n\n[1] train-rmse:16.864020 \n[2] train-rmse:12.196387 \n[3] train-rmse:8.915546 \n[4] train-rmse:6.600957 \n[5] train-rmse:4.953999 \n[6] train-rmse:3.785458 \n[7] train-rmse:2.948327 \n[8] train-rmse:2.351052 \n[9] train-rmse:1.895368 \n[10]    train-rmse:1.599467 \n[11]    train-rmse:1.400130 \n[12]    train-rmse:1.196252 \n[13]    train-rmse:1.092092 \n[14]    train-rmse:1.019197 \n[15]    train-rmse:0.941965 \n[16]    train-rmse:0.891416 \n[17]    train-rmse:0.844929 \n[18]    train-rmse:0.809411 \n[19]    train-rmse:0.775116 \n[20]    train-rmse:0.702806 \n[21]    train-rmse:0.663042 \n[22]    train-rmse:0.634505 \n[23]    train-rmse:0.574013 \n[24]    train-rmse:0.557150 \n[25]    train-rmse:0.512364 \n[26]    train-rmse:0.472643 \n[27]    train-rmse:0.449644 \n[28]    train-rmse:0.420094 \n[29]    train-rmse:0.407141 \n[30]    train-rmse:0.377183 \n[31]    train-rmse:0.347750 \n[32]    train-rmse:0.324683 \n[33]    train-rmse:0.311104 \n[34]    train-rmse:0.288693 \n[35]    train-rmse:0.267527 \n[36]    train-rmse:0.241400 \n[37]    train-rmse:0.223405 \n[38]    train-rmse:0.216449 \n[39]    train-rmse:0.200252 \n[40]    train-rmse:0.193583 \n[41]    train-rmse:0.181290 \n[42]    train-rmse:0.165571 \n[43]    train-rmse:0.159072 \n[44]    train-rmse:0.150027 \n[45]    train-rmse:0.140853 \n[46]    train-rmse:0.126037 \n[47]    train-rmse:0.123955 \n[48]    train-rmse:0.119162 \n[49]    train-rmse:0.114289 \n[50]    train-rmse:0.104554 \n[51]    train-rmse:0.102517 \n[52]    train-rmse:0.098968 \n[53]    train-rmse:0.095341 \n[54]    train-rmse:0.087070 \n[55]    train-rmse:0.083275 \n[56]    train-rmse:0.078488 \n[57]    train-rmse:0.075294 \n[58]    train-rmse:0.072737 \n[59]    train-rmse:0.069200 \n[60]    train-rmse:0.067622 \n[61]    train-rmse:0.061438 \n[62]    train-rmse:0.059305 \n[63]    train-rmse:0.057915 \n[64]    train-rmse:0.055674 \n[65]    train-rmse:0.051587 \n[66]    train-rmse:0.048178 \n[67]    train-rmse:0.046609 \n[68]    train-rmse:0.045095 \n[69]    train-rmse:0.042064 \n[70]    train-rmse:0.040559 \n[71]    train-rmse:0.038290 \n[72]    train-rmse:0.036909 \n[73]    train-rmse:0.034672 \n[74]    train-rmse:0.032852 \n[75]    train-rmse:0.031132 \n[76]    train-rmse:0.029724 \n[77]    train-rmse:0.027688 \n[78]    train-rmse:0.026797 \n[79]    train-rmse:0.025369 \n[80]    train-rmse:0.024092 \n[81]    train-rmse:0.023262 \n[82]    train-rmse:0.022374 \n[83]    train-rmse:0.021797 \n[84]    train-rmse:0.021347 \n[85]    train-rmse:0.019702 \n[86]    train-rmse:0.019364 \n[87]    train-rmse:0.017645 \n[88]    train-rmse:0.016802 \n[89]    train-rmse:0.016005 \n[90]    train-rmse:0.015056 \n[91]    train-rmse:0.014366 \n[92]    train-rmse:0.012951 \n[93]    train-rmse:0.012659 \n[94]    train-rmse:0.012012 \n[95]    train-rmse:0.011531 \n[96]    train-rmse:0.011218 \n[97]    train-rmse:0.010646 \n[98]    train-rmse:0.009803 \n[99]    train-rmse:0.008940 \n[100]   train-rmse:0.008829 \n\npredictions_xgb &lt;- predict(xgb_model, as.matrix(test_data[, colnames(test_data) != \"MEDV\"]))\nmse_xgb &lt;- mean((predictions_xgb - test_data$MEDV)^2)\n\n# Model B: Random Forest\nrf_model &lt;- randomForest(MEDV ~ ., data = train_data, ntree = 100)\npredictions_rf &lt;- predict(rf_model, newdata = test_data)\nmse_rf &lt;- mean((predictions_rf - test_data$MEDV)^2)\n\n# Comparative Analysis\nprint(\"XGBoost Model Metrics:\")\n\n[1] \"XGBoost Model Metrics:\"\n\nprint(paste(\"Mean Squared Error:\", mse_xgb))\n\n[1] \"Mean Squared Error: 9.55062801978429\"\n\nprint(\"Random Forest Model Metrics:\")\n\n[1] \"Random Forest Model Metrics:\"\n\nprint(paste(\"Mean Squared Error:\", mse_rf))\n\n[1] \"Mean Squared Error: 9.59836399135771\"\n\n# Comparative Scatter Plots\nplot_xgb &lt;- ggplot(data = test_data, aes(x = MEDV, y = predictions_xgb)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"XGBoost - Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\nplot_rf &lt;- ggplot(data = test_data, aes(x = MEDV, y = predictions_rf)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Random Forest - Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n# Display Plots\nprint(\"Comparative Scatter Plots:\")\n\n[1] \"Comparative Scatter Plots:\"\n\nprint(plot_xgb)\n\n\n\nprint(plot_rf)"
  },
  {
    "objectID": "posts/exploratory data analysis/index.html",
    "href": "posts/exploratory data analysis/index.html",
    "title": "Wine dataset exploratory data analysis",
    "section": "",
    "text": "# Load required libraries\nlibrary(ggplot2)\n\n# Load the dataset\nwine_data &lt;- read.csv(\"C:/Users/prasad/Downloads/WineQT.csv\")\n\n# Explore the structure of the dataset\nstr(wine_data)\n\n'data.frame':   1143 obs. of  13 variables:\n $ fixed.acidity       : num  7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 6.7 ...\n $ volatile.acidity    : num  0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.58 ...\n $ citric.acid         : num  0 0 0.04 0.56 0 0 0.06 0 0.02 0.08 ...\n $ residual.sugar      : num  1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 1.8 ...\n $ chlorides           : num  0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.097 ...\n $ free.sulfur.dioxide : num  11 25 15 17 11 13 15 15 9 15 ...\n $ total.sulfur.dioxide: num  34 67 54 60 34 40 59 21 18 65 ...\n $ density             : num  0.998 0.997 0.997 0.998 0.998 ...\n $ pH                  : num  3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.28 ...\n $ sulphates           : num  0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.54 ...\n $ alcohol             : num  9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 9.2 ...\n $ quality             : int  5 5 5 6 5 5 5 7 7 5 ...\n $ Id                  : int  0 1 2 3 4 5 6 7 8 10 ...\n\n# Display summary statistics\nsummary(wine_data)\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 4.600   Min.   :0.1200   Min.   :0.0000   Min.   : 0.900  \n 1st Qu.: 7.100   1st Qu.:0.3925   1st Qu.:0.0900   1st Qu.: 1.900  \n Median : 7.900   Median :0.5200   Median :0.2500   Median : 2.200  \n Mean   : 8.311   Mean   :0.5313   Mean   :0.2684   Mean   : 2.532  \n 3rd Qu.: 9.100   3rd Qu.:0.6400   3rd Qu.:0.4200   3rd Qu.: 2.600  \n Max.   :15.900   Max.   :1.5800   Max.   :1.0000   Max.   :15.500  \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      \n Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  \n 1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 21.00       1st Qu.:0.9956  \n Median :0.07900   Median :13.00       Median : 37.00       Median :0.9967  \n Mean   :0.08693   Mean   :15.62       Mean   : 45.91       Mean   :0.9967  \n 3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 61.00       3rd Qu.:0.9978  \n Max.   :0.61100   Max.   :68.00       Max.   :289.00       Max.   :1.0037  \n       pH          sulphates         alcohol         quality     \n Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000  \n 1st Qu.:3.205   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.310   Median :0.6200   Median :10.20   Median :6.000  \n Mean   :3.311   Mean   :0.6577   Mean   :10.44   Mean   :5.657  \n 3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  \n Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000  \n       Id      \n Min.   :   0  \n 1st Qu.: 411  \n Median : 794  \n Mean   : 805  \n 3rd Qu.:1210  \n Max.   :1597  \n\n# Check for missing values\nsum(is.na(wine_data))\n\n[1] 0\n\n# Data visualization for each feature\n# You can customize the plots based on your preferences and requirements\n\n# Box plots for each input variable\nfor (col in 1:11) {\n  ggplot(wine_data, aes(x = quality, y = wine_data[, col])) +\n    geom_boxplot(fill = \"skyblue\", color = \"steelblue\") +\n    labs(title = paste(\"Box plot of\", names(wine_data)[col]))\n}\n\n# Correlation matrix\ncor_matrix &lt;- cor(wine_data[, 1:11])\nggplot(data = as.data.frame(as.table(cor_matrix)), aes(x = Var1, y = Var2, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1, 1), space = \"Lab\",\n                       name = \"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1))\n\n\n\n# Histogram of the target variable (quality)\nggplot(wine_data, aes(x = quality)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"steelblue\") +\n  labs(title = \"Histogram of Wine Quality\")\n\n\n\n# Class distribution bar plot\nggplot(wine_data, aes(x = as.factor(quality), fill = as.factor(quality))) +\n  geom_bar() +\n  labs(title = \"Class Distribution of Wine Quality\")\n\n\n\n# Pair plots for a subset of variables\n# You can customize the subset based on your preferences\nsubset_vars &lt;- c(\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"quality\")\n\n# Replace spaces in column names with dots and use backticks\ncolnames(wine_data) &lt;- gsub(\" \", \".\", colnames(wine_data))\n\n# Check the column names to ensure they match with the actual names in your dataset\ncolnames(wine_data)\n\n [1] \"fixed.acidity\"        \"volatile.acidity\"     \"citric.acid\"         \n [4] \"residual.sugar\"       \"chlorides\"            \"free.sulfur.dioxide\" \n [7] \"total.sulfur.dioxide\" \"density\"              \"pH\"                  \n[10] \"sulphates\"            \"alcohol\"              \"quality\"             \n[13] \"Id\"                  \n\n# Subset of variables for pair plots\nsubset_vars &lt;- c(\"fixed.acidity\", \"volatile.acidity\", \"citric.acid\", \"residual.sugar\", \"quality\")\n\n# Scatter plot matrix for a subset of variables\npairs(wine_data[, subset_vars], pch = 16, col = as.factor(wine_data$quality))\n\n\n\n# Correlation heatmap\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.3.2\n\n\ncorrplot 0.92 loaded\n\ncorrplot(cor(wine_data[, 1:11]), method = \"color\")\n\n\n\n\nThe summary statistics provide insights into the key characteristics of the dataset related to red variants of Portuguese “Vinho Verde” wine. The physicochemical properties of the wines vary across the sampled instances. The mean fixed acidity is 8.311, with a range from 4.6 to 15.9. Volatile acidity ranges from 0.12 to 1.58, with a mean of 0.5313. Citric acid content ranges from 0 to 1, with a mean of 0.2684. Residual sugar has a mean of 2.532 and ranges from 0.9 to 15.5. Chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol also exhibit varying ranges and means. The quality scores, ranging from 3 to 8, have a mean of 5.657. The dataset comprises 1598 instances, with an ‘Id’ column ranging from 0 to 1597. The summary provides a concise overview of the dataset’s central tendencies and variability, laying the foundation for further exploration and modeling."
  }
]