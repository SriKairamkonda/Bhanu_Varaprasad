{
  "hash": "73f4602265eed574214e59ab6dce1f47",
  "result": {
    "markdown": "---\ntitle: \"Housing data analysis\"\nauthor: \"Bhanu Varaprasad\"\ndate: \"2023-12-08\"\ncategories: [news, code, analysis,plotly,plot]\nimage: \"profile.jpg\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(reshape2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'reshape2' was built under R version 4.3.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'reshape2'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tidyr':\n\n    smiths\n```\n:::\n\n```{.r .cell-code}\n# Load the dataset\nHousingData <- read.csv(\"C:/Users/YOJANA/Downloads/AN/KN/HousingData.csv\")\n\n# Display the structure of the dataset\nstr(HousingData)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t506 obs. of  14 variables:\n $ CRIM   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ ZN     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ INDUS  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ CHAS   : int  0 0 0 0 0 0 NA 0 0 NA ...\n $ NOX    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ RM     : num  6.58 6.42 7.18 7 7.15 ...\n $ AGE    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ DIS    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ RAD    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ TAX    : int  296 242 242 222 222 222 311 311 311 311 ...\n $ PTRATIO: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ B      : num  397 397 393 395 397 ...\n $ LSTAT  : num  4.98 9.14 4.03 2.94 NA ...\n $ MEDV   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n```\n:::\n\n```{.r .cell-code}\n# Display summary statistics\nsummary(HousingData)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      CRIM                ZN             INDUS            CHAS        \n Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n 1st Qu.: 0.08190   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n Median : 0.25372   Median :  0.00   Median : 9.69   Median :0.00000  \n Mean   : 3.61187   Mean   : 11.21   Mean   :11.08   Mean   :0.06996  \n 3rd Qu.: 3.56026   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n NA's   :20         NA's   :20       NA's   :20      NA's   :20       \n      NOX               RM             AGE              DIS        \n Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n 1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.17   1st Qu.: 2.100  \n Median :0.5380   Median :6.208   Median : 76.80   Median : 3.207  \n Mean   :0.5547   Mean   :6.285   Mean   : 68.52   Mean   : 3.795  \n 3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 93.97   3rd Qu.: 5.188  \n Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n                                  NA's   :20                       \n      RAD              TAX           PTRATIO            B         \n Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32  \n 1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38  \n Median : 5.000   Median :330.0   Median :19.05   Median :391.44  \n Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67  \n 3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23  \n Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90  \n                                                                  \n     LSTAT             MEDV      \n Min.   : 1.730   Min.   : 5.00  \n 1st Qu.: 7.125   1st Qu.:17.02  \n Median :11.430   Median :21.20  \n Mean   :12.715   Mean   :22.53  \n 3rd Qu.:16.955   3rd Qu.:25.00  \n Max.   :37.970   Max.   :50.00  \n NA's   :20                      \n```\n:::\n\n```{.r .cell-code}\n# Check for missing values\nsum(is.na(HousingData))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 120\n```\n:::\n\n```{.r .cell-code}\n# Explore the distribution of the target variable (MEDV)\nggplot(HousingData, aes(x = MEDV)) +\n  geom_histogram(fill = \"blue\", bins = 30) +\n  labs(title = \"Distribution of Median Home Values\",\n       x = \"Median Home Value ($1000's)\",\n       y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Explore relationships between variables using scatter plots\npairs(HousingData[, c(\"CRIM\", \"RM\", \"AGE\", \"DIS\", \"TAX\", \"LSTAT\", \"MEDV\")])\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Correlation matrix to identify relationships between variables\ncor_matrix <- cor(HousingData)\n\n# Display a heatmap of the correlation matrix\nggplot(data = melt(cor_matrix), aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Correlation Heatmap\",\n       x = \"Variables\",\n       y = \"Variables\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Boxplots to visualize the distribution of variables\npar(mfrow = c(4, 4), mar = c(2, 2, 2, 2))  # Adjust margin parameters\nfor (i in 1:14) {\n  boxplot(HousingData[, i], main = colnames(HousingData)[i], col = \"lightblue\")\n}\n\n# Explore relationships between selected variables\nggplot(HousingData, aes(x = RM, y = MEDV)) +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"Scatter Plot of RM vs. MEDV\",\n       x = \"Average Number of Rooms\",\n       y = \"Median Home Value ($1000's)\")\n\n# Conduct more exploratory data analysis based on your specific questions and hypotheses\n\n# Example: Analyzing the impact of crime rate on median home values\nggplot(HousingData, aes(x = CRIM, y = MEDV)) +\n  geom_point(alpha = 0.6, color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(title = \"Impact of Crime Rate on Median Home Values\",\n       x = \"Per Capita Crime Rate\",\n       y = \"Median Home Value ($1000's)\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 20 rows containing non-finite values (`stat_smooth()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 20 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-4.png){width=672}\n:::\n:::\n\n\n***`Exploratory Data Analysis (EDA) is a crucial step in understanding and gaining insights from a dataset. In the case of the Boston Housing Dataset, the summary statistics reveal the variability and distribution of key features such as crime rate (CRIM), proportion of residential land (ZN), and non-retail business acres (INDUS). The dataset contains information about factors like air quality (NOX), average number of rooms (RM), and socio-economic status (LSTAT), which may influence the median value of owner-occupied homes (MEDV). Handling missing values is essential, and in this dataset, 20 rows have missing entries in various columns. Through graphical representations such as histograms and scatter plots, relationships between variables can be observed, aiding in the identification of patterns and potential outliers. The dataset provides a comprehensive view of diverse factors influencing housing values, setting the stage for further in-depth analysis and modeling.`***\n\n***`The correlation matrix provides insights into the relationships between various features in the dataset. Examining the correlation matrix for the Boston Housing Dataset, it is evident that certain pairs of variables exhibit notable correlations. For instance, the variable \"RM,\" representing the average number of rooms per dwelling, displays a positive correlation with the target variable \"MEDV,\" indicating a higher median home value with an increased number of rooms. Conversely, the \"LSTAT\" variable, representing the percentage of the lower status of the population, demonstrates a negative correlation with \"MEDV,\" suggesting that areas with a higher percentage of lower-status residents tend to have lower median home values. These insights from the correlation matrix lay the foundation for further exploration and model building, guiding the selection of relevant features for predictive analysis in the Boston Housing Dataset.`***\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load the required packages\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'randomForest' was built under R version 4.3.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Load the dataset\ndata <- read.csv(\"C:/Users/YOJANA/Downloads/AN/KN/HousingData.csv\")\n\n# Remove rows with missing values\ndata <- na.omit(data)\n\n# Separate features (X) and target variable (y)\nX <- data[, colnames(data) != \"MEDV\"]\ny <- data$MEDV\n\n# Split the data into training and testing sets\nset.seed(42)\nindices <- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data <- data[indices, ]\ntest_data <- data[-indices, ]\n\n# Initialize the Random Forest Regressor\nrf_model <- randomForest(MEDV ~ ., data = train_data, ntree = 100)\n\n# Make predictions on the test set\npredictions <- predict(rf_model, newdata = test_data)\n\n# Evaluate the model\nmse <- mean((predictions - test_data$MEDV)^2)\nr_squared <- 1 - mse / var(test_data$MEDV)\n\nprint(paste(\"Mean Squared Error:\", mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Mean Squared Error: 10.243041067851\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"R-squared:\", r_squared))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"R-squared: 0.897607585182528\"\n```\n:::\n\n```{.r .cell-code}\n# Visualization: Scatter plot of actual vs. predicted values\nggplot(data = test_data, aes(x = MEDV, y = predictions)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n***The machine learning model, utilizing a Random Forest Regressor with 100 trees, demonstrated promising performance on the Boston Housing Dataset. The evaluation metrics indicate a mean squared error (MSE) of 10.24 and an R-squared value of 0.90 on the test set. The MSE measures the average squared difference between the predicted and actual median values of owner-occupied homes, providing a quantitative assessment of the model's accuracy. In this context, the relatively low MSE signifies that the model's predictions are, on average, close to the true values. Additionally, the high R-squared value of 0.90 suggests that the majority of the variance in the target variable is explained by the model, indicating a strong predictive capability. These results imply that the Random Forest Regressor has effectively captured the complex relationships within the dataset, providing a reliable framework for predicting median home values based on the given features.***\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load the required packages\nlibrary(pdp)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'pdp' was built under R version 4.3.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(xgboost)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'xgboost' was built under R version 4.3.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'xgboost'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Load the dataset\ndata <- read.csv(\"C:/Users/YOJANA/Downloads/AN/KN/HousingData.csv\")\n\n# Remove rows with missing values\ndata <- na.omit(data)\n\n# Separate features (X) and target variable (y)\nX <- data[, colnames(data) != \"MEDV\"]\ny <- data$MEDV\n\n# Split the data into training and testing sets\nset.seed(42)\nindices <- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data <- data[indices, ]\ntest_data <- data[-indices, ]\n\n# Initialize the XGBoost Regressor\nxgb_model <- xgboost(data = as.matrix(train_data[, colnames(train_data) != \"MEDV\"]), label = train_data$MEDV, nrounds = 100, objective = \"reg:squarederror\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\ttrain-rmse:16.864020 \n[2]\ttrain-rmse:12.196387 \n[3]\ttrain-rmse:8.915546 \n[4]\ttrain-rmse:6.600957 \n[5]\ttrain-rmse:4.953999 \n[6]\ttrain-rmse:3.785458 \n[7]\ttrain-rmse:2.948327 \n[8]\ttrain-rmse:2.351052 \n[9]\ttrain-rmse:1.895368 \n[10]\ttrain-rmse:1.599467 \n[11]\ttrain-rmse:1.400130 \n[12]\ttrain-rmse:1.196252 \n[13]\ttrain-rmse:1.092092 \n[14]\ttrain-rmse:1.019197 \n[15]\ttrain-rmse:0.941965 \n[16]\ttrain-rmse:0.891416 \n[17]\ttrain-rmse:0.844929 \n[18]\ttrain-rmse:0.809411 \n[19]\ttrain-rmse:0.775116 \n[20]\ttrain-rmse:0.702806 \n[21]\ttrain-rmse:0.663042 \n[22]\ttrain-rmse:0.634505 \n[23]\ttrain-rmse:0.574013 \n[24]\ttrain-rmse:0.557150 \n[25]\ttrain-rmse:0.512364 \n[26]\ttrain-rmse:0.472643 \n[27]\ttrain-rmse:0.449644 \n[28]\ttrain-rmse:0.420094 \n[29]\ttrain-rmse:0.407141 \n[30]\ttrain-rmse:0.377183 \n[31]\ttrain-rmse:0.347750 \n[32]\ttrain-rmse:0.324683 \n[33]\ttrain-rmse:0.311104 \n[34]\ttrain-rmse:0.288693 \n[35]\ttrain-rmse:0.267527 \n[36]\ttrain-rmse:0.241400 \n[37]\ttrain-rmse:0.223405 \n[38]\ttrain-rmse:0.216449 \n[39]\ttrain-rmse:0.200252 \n[40]\ttrain-rmse:0.193583 \n[41]\ttrain-rmse:0.181290 \n[42]\ttrain-rmse:0.165571 \n[43]\ttrain-rmse:0.159072 \n[44]\ttrain-rmse:0.150027 \n[45]\ttrain-rmse:0.140853 \n[46]\ttrain-rmse:0.126037 \n[47]\ttrain-rmse:0.123955 \n[48]\ttrain-rmse:0.119162 \n[49]\ttrain-rmse:0.114289 \n[50]\ttrain-rmse:0.104554 \n[51]\ttrain-rmse:0.102517 \n[52]\ttrain-rmse:0.098968 \n[53]\ttrain-rmse:0.095341 \n[54]\ttrain-rmse:0.087070 \n[55]\ttrain-rmse:0.083275 \n[56]\ttrain-rmse:0.078488 \n[57]\ttrain-rmse:0.075294 \n[58]\ttrain-rmse:0.072737 \n[59]\ttrain-rmse:0.069200 \n[60]\ttrain-rmse:0.067622 \n[61]\ttrain-rmse:0.061438 \n[62]\ttrain-rmse:0.059305 \n[63]\ttrain-rmse:0.057915 \n[64]\ttrain-rmse:0.055674 \n[65]\ttrain-rmse:0.051587 \n[66]\ttrain-rmse:0.048178 \n[67]\ttrain-rmse:0.046609 \n[68]\ttrain-rmse:0.045095 \n[69]\ttrain-rmse:0.042064 \n[70]\ttrain-rmse:0.040559 \n[71]\ttrain-rmse:0.038290 \n[72]\ttrain-rmse:0.036909 \n[73]\ttrain-rmse:0.034672 \n[74]\ttrain-rmse:0.032852 \n[75]\ttrain-rmse:0.031132 \n[76]\ttrain-rmse:0.029724 \n[77]\ttrain-rmse:0.027688 \n[78]\ttrain-rmse:0.026797 \n[79]\ttrain-rmse:0.025369 \n[80]\ttrain-rmse:0.024092 \n[81]\ttrain-rmse:0.023262 \n[82]\ttrain-rmse:0.022374 \n[83]\ttrain-rmse:0.021797 \n[84]\ttrain-rmse:0.021347 \n[85]\ttrain-rmse:0.019702 \n[86]\ttrain-rmse:0.019364 \n[87]\ttrain-rmse:0.017645 \n[88]\ttrain-rmse:0.016802 \n[89]\ttrain-rmse:0.016005 \n[90]\ttrain-rmse:0.015056 \n[91]\ttrain-rmse:0.014366 \n[92]\ttrain-rmse:0.012951 \n[93]\ttrain-rmse:0.012659 \n[94]\ttrain-rmse:0.012012 \n[95]\ttrain-rmse:0.011531 \n[96]\ttrain-rmse:0.011218 \n[97]\ttrain-rmse:0.010646 \n[98]\ttrain-rmse:0.009803 \n[99]\ttrain-rmse:0.008940 \n[100]\ttrain-rmse:0.008829 \n```\n:::\n\n```{.r .cell-code}\n# Make predictions on the test set\npredictions <- predict(xgb_model, as.matrix(test_data[, colnames(test_data) != \"MEDV\"]))\n\n# Evaluate the model\nmse <- mean((predictions - test_data$MEDV)^2)\nr_squared <- 1 - mse / var(test_data$MEDV)\n\nprint(paste(\"Mean Squared Error:\", mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Mean Squared Error: 9.55062801978429\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"R-squared:\", r_squared))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"R-squared: 0.9045291472043\"\n```\n:::\n\n```{.r .cell-code}\n# Visualization 1: Scatter plot of actual vs. predicted values\nggplot(data = test_data, aes(x = MEDV, y = predictions)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Visualization 2: Feature Importance Plot\nfeature_importance <- xgb.importance(model = xgb_model)\nxgb.plot.importance(importance_matrix = feature_importance)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n***`The XGBoost model demonstrates a notable reduction in training root mean squared error (train-rmse) throughout the training process, starting from 16.86 and progressively decreasing to 0.0088. The mean squared error on the test set is 9.55, indicating strong predictive performance. The high R-squared value of 0.9045 further confirms the model's ability to explain the variance in the target variable, suggesting that the XGBoost model effectively captures the underlying patterns in the housing data, making it a robust choice for regression tasks.`***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xgboost)\nlibrary(randomForest)\nlibrary(ggplot2)\n\n# Load and preprocess the data\ndata <- na.omit(read.csv(\"C:/Users/YOJANA/Downloads/AN/KN/HousingData.csv\"))\nset.seed(42)\nindices <- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data <- data[indices, ]\ntest_data <- data[-indices, ]\n\n# Model A: XGBoost\nxgb_model <- xgboost(data = as.matrix(train_data[, colnames(train_data) != \"MEDV\"]), label = train_data$MEDV, nrounds = 100, objective = \"reg:squarederror\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\ttrain-rmse:16.864020 \n[2]\ttrain-rmse:12.196387 \n[3]\ttrain-rmse:8.915546 \n[4]\ttrain-rmse:6.600957 \n[5]\ttrain-rmse:4.953999 \n[6]\ttrain-rmse:3.785458 \n[7]\ttrain-rmse:2.948327 \n[8]\ttrain-rmse:2.351052 \n[9]\ttrain-rmse:1.895368 \n[10]\ttrain-rmse:1.599467 \n[11]\ttrain-rmse:1.400130 \n[12]\ttrain-rmse:1.196252 \n[13]\ttrain-rmse:1.092092 \n[14]\ttrain-rmse:1.019197 \n[15]\ttrain-rmse:0.941965 \n[16]\ttrain-rmse:0.891416 \n[17]\ttrain-rmse:0.844929 \n[18]\ttrain-rmse:0.809411 \n[19]\ttrain-rmse:0.775116 \n[20]\ttrain-rmse:0.702806 \n[21]\ttrain-rmse:0.663042 \n[22]\ttrain-rmse:0.634505 \n[23]\ttrain-rmse:0.574013 \n[24]\ttrain-rmse:0.557150 \n[25]\ttrain-rmse:0.512364 \n[26]\ttrain-rmse:0.472643 \n[27]\ttrain-rmse:0.449644 \n[28]\ttrain-rmse:0.420094 \n[29]\ttrain-rmse:0.407141 \n[30]\ttrain-rmse:0.377183 \n[31]\ttrain-rmse:0.347750 \n[32]\ttrain-rmse:0.324683 \n[33]\ttrain-rmse:0.311104 \n[34]\ttrain-rmse:0.288693 \n[35]\ttrain-rmse:0.267527 \n[36]\ttrain-rmse:0.241400 \n[37]\ttrain-rmse:0.223405 \n[38]\ttrain-rmse:0.216449 \n[39]\ttrain-rmse:0.200252 \n[40]\ttrain-rmse:0.193583 \n[41]\ttrain-rmse:0.181290 \n[42]\ttrain-rmse:0.165571 \n[43]\ttrain-rmse:0.159072 \n[44]\ttrain-rmse:0.150027 \n[45]\ttrain-rmse:0.140853 \n[46]\ttrain-rmse:0.126037 \n[47]\ttrain-rmse:0.123955 \n[48]\ttrain-rmse:0.119162 \n[49]\ttrain-rmse:0.114289 \n[50]\ttrain-rmse:0.104554 \n[51]\ttrain-rmse:0.102517 \n[52]\ttrain-rmse:0.098968 \n[53]\ttrain-rmse:0.095341 \n[54]\ttrain-rmse:0.087070 \n[55]\ttrain-rmse:0.083275 \n[56]\ttrain-rmse:0.078488 \n[57]\ttrain-rmse:0.075294 \n[58]\ttrain-rmse:0.072737 \n[59]\ttrain-rmse:0.069200 \n[60]\ttrain-rmse:0.067622 \n[61]\ttrain-rmse:0.061438 \n[62]\ttrain-rmse:0.059305 \n[63]\ttrain-rmse:0.057915 \n[64]\ttrain-rmse:0.055674 \n[65]\ttrain-rmse:0.051587 \n[66]\ttrain-rmse:0.048178 \n[67]\ttrain-rmse:0.046609 \n[68]\ttrain-rmse:0.045095 \n[69]\ttrain-rmse:0.042064 \n[70]\ttrain-rmse:0.040559 \n[71]\ttrain-rmse:0.038290 \n[72]\ttrain-rmse:0.036909 \n[73]\ttrain-rmse:0.034672 \n[74]\ttrain-rmse:0.032852 \n[75]\ttrain-rmse:0.031132 \n[76]\ttrain-rmse:0.029724 \n[77]\ttrain-rmse:0.027688 \n[78]\ttrain-rmse:0.026797 \n[79]\ttrain-rmse:0.025369 \n[80]\ttrain-rmse:0.024092 \n[81]\ttrain-rmse:0.023262 \n[82]\ttrain-rmse:0.022374 \n[83]\ttrain-rmse:0.021797 \n[84]\ttrain-rmse:0.021347 \n[85]\ttrain-rmse:0.019702 \n[86]\ttrain-rmse:0.019364 \n[87]\ttrain-rmse:0.017645 \n[88]\ttrain-rmse:0.016802 \n[89]\ttrain-rmse:0.016005 \n[90]\ttrain-rmse:0.015056 \n[91]\ttrain-rmse:0.014366 \n[92]\ttrain-rmse:0.012951 \n[93]\ttrain-rmse:0.012659 \n[94]\ttrain-rmse:0.012012 \n[95]\ttrain-rmse:0.011531 \n[96]\ttrain-rmse:0.011218 \n[97]\ttrain-rmse:0.010646 \n[98]\ttrain-rmse:0.009803 \n[99]\ttrain-rmse:0.008940 \n[100]\ttrain-rmse:0.008829 \n```\n:::\n\n```{.r .cell-code}\npredictions_xgb <- predict(xgb_model, as.matrix(test_data[, colnames(test_data) != \"MEDV\"]))\nmse_xgb <- mean((predictions_xgb - test_data$MEDV)^2)\n\n# Model B: Random Forest\nrf_model <- randomForest(MEDV ~ ., data = train_data, ntree = 100)\npredictions_rf <- predict(rf_model, newdata = test_data)\nmse_rf <- mean((predictions_rf - test_data$MEDV)^2)\n\n# Comparative Analysis\nprint(\"XGBoost Model Metrics:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"XGBoost Model Metrics:\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Mean Squared Error:\", mse_xgb))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Mean Squared Error: 9.55062801978429\"\n```\n:::\n\n```{.r .cell-code}\nprint(\"Random Forest Model Metrics:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Random Forest Model Metrics:\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Mean Squared Error:\", mse_rf))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Mean Squared Error: 9.59836399135771\"\n```\n:::\n\n```{.r .cell-code}\n# Comparative Scatter Plots\nplot_xgb <- ggplot(data = test_data, aes(x = MEDV, y = predictions_xgb)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"XGBoost - Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\nplot_rf <- ggplot(data = test_data, aes(x = MEDV, y = predictions_rf)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Random Forest - Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n# Display Plots\nprint(\"Comparative Scatter Plots:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Comparative Scatter Plots:\"\n```\n:::\n\n```{.r .cell-code}\nprint(plot_xgb)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(plot_rf)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}