{"title":"Unveiling the Marvels of Data Science: A Journey into the Heart of Information","markdown":{"yaml":{"title":"Unveiling the Marvels of Data Science: A Journey into the Heart of Information","author":"Bhanu Varaprasad","date":"2023-12-08","categories":["news","code","analysis","plotly","plot"],"image":"profile.jpg"},"headingText":"Load necessary libraries","containsRefs":false,"markdown":"\n\n```{r}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reshape2)\n\n# Load the dataset\nHousingData <- read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\")\n\n# Display the structure of the dataset\nstr(HousingData)\n\n# Display summary statistics\nsummary(HousingData)\n\n# Check for missing values\nsum(is.na(HousingData))\n\n# Explore the distribution of the target variable (MEDV)\nggplot(HousingData, aes(x = MEDV)) +\n  geom_histogram(fill = \"blue\", bins = 30) +\n  labs(title = \"Distribution of Median Home Values\",\n       x = \"Median Home Value ($1000's)\",\n       y = \"Frequency\")\n\n# Explore relationships between variables using scatter plots\npairs(HousingData[, c(\"CRIM\", \"RM\", \"AGE\", \"DIS\", \"TAX\", \"LSTAT\", \"MEDV\")])\n\n# Correlation matrix to identify relationships between variables\ncor_matrix <- cor(HousingData)\n\n# Display a heatmap of the correlation matrix\nggplot(data = melt(cor_matrix), aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Correlation Heatmap\",\n       x = \"Variables\",\n       y = \"Variables\")\n\n# Boxplots to visualize the distribution of variables\npar(mfrow = c(4, 4), mar = c(2, 2, 2, 2))  # Adjust margin parameters\nfor (i in 1:14) {\n  boxplot(HousingData[, i], main = colnames(HousingData)[i], col = \"lightblue\")\n}\n\n# Explore relationships between selected variables\nggplot(HousingData, aes(x = RM, y = MEDV)) +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"Scatter Plot of RM vs. MEDV\",\n       x = \"Average Number of Rooms\",\n       y = \"Median Home Value ($1000's)\")\n\n# Conduct more exploratory data analysis based on your specific questions and hypotheses\n\n# Example: Analyzing the impact of crime rate on median home values\nggplot(HousingData, aes(x = CRIM, y = MEDV)) +\n  geom_point(alpha = 0.6, color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(title = \"Impact of Crime Rate on Median Home Values\",\n       x = \"Per Capita Crime Rate\",\n       y = \"Median Home Value ($1000's)\")\n```\n\n***`Exploratory Data Analysis (EDA) is a crucial step in understanding and gaining insights from a dataset. In the case of the Boston Housing Dataset, the summary statistics reveal the variability and distribution of key features such as crime rate (CRIM), proportion of residential land (ZN), and non-retail business acres (INDUS). The dataset contains information about factors like air quality (NOX), average number of rooms (RM), and socio-economic status (LSTAT), which may influence the median value of owner-occupied homes (MEDV). Handling missing values is essential, and in this dataset, 20 rows have missing entries in various columns. Through graphical representations such as histograms and scatter plots, relationships between variables can be observed, aiding in the identification of patterns and potential outliers. The dataset provides a comprehensive view of diverse factors influencing housing values, setting the stage for further in-depth analysis and modeling.`***\n\n***`The correlation matrix provides insights into the relationships between various features in the dataset. Examining the correlation matrix for the Boston Housing Dataset, it is evident that certain pairs of variables exhibit notable correlations. For instance, the variable \"RM,\" representing the average number of rooms per dwelling, displays a positive correlation with the target variable \"MEDV,\" indicating a higher median home value with an increased number of rooms. Conversely, the \"LSTAT\" variable, representing the percentage of the lower status of the population, demonstrates a negative correlation with \"MEDV,\" suggesting that areas with a higher percentage of lower-status residents tend to have lower median home values. These insights from the correlation matrix lay the foundation for further exploration and model building, guiding the selection of relevant features for predictive analysis in the Boston Housing Dataset.`***\n\n------------------------------------------------------------------------\n\n```{r}\n# Install and load the required packages\nlibrary(randomForest)\nlibrary(ggplot2)\n\n# Load the dataset\ndata <- read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\")\n\n# Remove rows with missing values\ndata <- na.omit(data)\n\n# Separate features (X) and target variable (y)\nX <- data[, colnames(data) != \"MEDV\"]\ny <- data$MEDV\n\n# Split the data into training and testing sets\nset.seed(42)\nindices <- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data <- data[indices, ]\ntest_data <- data[-indices, ]\n\n# Initialize the Random Forest Regressor\nrf_model <- randomForest(MEDV ~ ., data = train_data, ntree = 100)\n\n# Make predictions on the test set\npredictions <- predict(rf_model, newdata = test_data)\n\n# Evaluate the model\nmse <- mean((predictions - test_data$MEDV)^2)\nr_squared <- 1 - mse / var(test_data$MEDV)\n\nprint(paste(\"Mean Squared Error:\", mse))\nprint(paste(\"R-squared:\", r_squared))\n\n# Visualization: Scatter plot of actual vs. predicted values\nggplot(data = test_data, aes(x = MEDV, y = predictions)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n```\n\n***The machine learning model, utilizing a Random Forest Regressor with 100 trees, demonstrated promising performance on the Boston Housing Dataset. The evaluation metrics indicate a mean squared error (MSE) of 10.24 and an R-squared value of 0.90 on the test set. The MSE measures the average squared difference between the predicted and actual median values of owner-occupied homes, providing a quantitative assessment of the model's accuracy. In this context, the relatively low MSE signifies that the model's predictions are, on average, close to the true values. Additionally, the high R-squared value of 0.90 suggests that the majority of the variance in the target variable is explained by the model, indicating a strong predictive capability. These results imply that the Random Forest Regressor has effectively captured the complex relationships within the dataset, providing a reliable framework for predicting median home values based on the given features.***\n\n------------------------------------------------------------------------\n\n```{r}\n# Install and load the required packages\nlibrary(pdp)\nlibrary(xgboost)\nlibrary(ggplot2)\n\n# Load the dataset\ndata <- read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\")\n\n# Remove rows with missing values\ndata <- na.omit(data)\n\n# Separate features (X) and target variable (y)\nX <- data[, colnames(data) != \"MEDV\"]\ny <- data$MEDV\n\n# Split the data into training and testing sets\nset.seed(42)\nindices <- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data <- data[indices, ]\ntest_data <- data[-indices, ]\n\n# Initialize the XGBoost Regressor\nxgb_model <- xgboost(data = as.matrix(train_data[, colnames(train_data) != \"MEDV\"]), label = train_data$MEDV, nrounds = 100, objective = \"reg:squarederror\")\n\n# Make predictions on the test set\npredictions <- predict(xgb_model, as.matrix(test_data[, colnames(test_data) != \"MEDV\"]))\n\n# Evaluate the model\nmse <- mean((predictions - test_data$MEDV)^2)\nr_squared <- 1 - mse / var(test_data$MEDV)\n\nprint(paste(\"Mean Squared Error:\", mse))\nprint(paste(\"R-squared:\", r_squared))\n\n# Visualization 1: Scatter plot of actual vs. predicted values\nggplot(data = test_data, aes(x = MEDV, y = predictions)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n# Visualization 2: Feature Importance Plot\nfeature_importance <- xgb.importance(model = xgb_model)\nxgb.plot.importance(importance_matrix = feature_importance)\n\n```\n\n***`The XGBoost model demonstrates a notable reduction in training root mean squared error (train-rmse) throughout the training process, starting from 16.86 and progressively decreasing to 0.0088. The mean squared error on the test set is 9.55, indicating strong predictive performance. The high R-squared value of 0.9045 further confirms the model's ability to explain the variance in the target variable, suggesting that the XGBoost model effectively captures the underlying patterns in the housing data, making it a robust choice for regression tasks.`***\n\n```{r}\nlibrary(xgboost)\nlibrary(randomForest)\nlibrary(ggplot2)\n\n# Load and preprocess the data\ndata <- na.omit(read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\"))\nset.seed(42)\nindices <- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data <- data[indices, ]\ntest_data <- data[-indices, ]\n\n# Model A: XGBoost\nxgb_model <- xgboost(data = as.matrix(train_data[, colnames(train_data) != \"MEDV\"]), label = train_data$MEDV, nrounds = 100, objective = \"reg:squarederror\")\npredictions_xgb <- predict(xgb_model, as.matrix(test_data[, colnames(test_data) != \"MEDV\"]))\nmse_xgb <- mean((predictions_xgb - test_data$MEDV)^2)\n\n# Model B: Random Forest\nrf_model <- randomForest(MEDV ~ ., data = train_data, ntree = 100)\npredictions_rf <- predict(rf_model, newdata = test_data)\nmse_rf <- mean((predictions_rf - test_data$MEDV)^2)\n\n# Comparative Analysis\nprint(\"XGBoost Model Metrics:\")\nprint(paste(\"Mean Squared Error:\", mse_xgb))\n\nprint(\"Random Forest Model Metrics:\")\nprint(paste(\"Mean Squared Error:\", mse_rf))\n\n# Comparative Scatter Plots\nplot_xgb <- ggplot(data = test_data, aes(x = MEDV, y = predictions_xgb)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"XGBoost - Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\nplot_rf <- ggplot(data = test_data, aes(x = MEDV, y = predictions_rf)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Random Forest - Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n# Display Plots\nprint(\"Comparative Scatter Plots:\")\nprint(plot_xgb)\nprint(plot_rf)\n```\n","srcMarkdownNoYaml":"\n\n```{r}\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reshape2)\n\n# Load the dataset\nHousingData <- read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\")\n\n# Display the structure of the dataset\nstr(HousingData)\n\n# Display summary statistics\nsummary(HousingData)\n\n# Check for missing values\nsum(is.na(HousingData))\n\n# Explore the distribution of the target variable (MEDV)\nggplot(HousingData, aes(x = MEDV)) +\n  geom_histogram(fill = \"blue\", bins = 30) +\n  labs(title = \"Distribution of Median Home Values\",\n       x = \"Median Home Value ($1000's)\",\n       y = \"Frequency\")\n\n# Explore relationships between variables using scatter plots\npairs(HousingData[, c(\"CRIM\", \"RM\", \"AGE\", \"DIS\", \"TAX\", \"LSTAT\", \"MEDV\")])\n\n# Correlation matrix to identify relationships between variables\ncor_matrix <- cor(HousingData)\n\n# Display a heatmap of the correlation matrix\nggplot(data = melt(cor_matrix), aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Correlation Heatmap\",\n       x = \"Variables\",\n       y = \"Variables\")\n\n# Boxplots to visualize the distribution of variables\npar(mfrow = c(4, 4), mar = c(2, 2, 2, 2))  # Adjust margin parameters\nfor (i in 1:14) {\n  boxplot(HousingData[, i], main = colnames(HousingData)[i], col = \"lightblue\")\n}\n\n# Explore relationships between selected variables\nggplot(HousingData, aes(x = RM, y = MEDV)) +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"Scatter Plot of RM vs. MEDV\",\n       x = \"Average Number of Rooms\",\n       y = \"Median Home Value ($1000's)\")\n\n# Conduct more exploratory data analysis based on your specific questions and hypotheses\n\n# Example: Analyzing the impact of crime rate on median home values\nggplot(HousingData, aes(x = CRIM, y = MEDV)) +\n  geom_point(alpha = 0.6, color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(title = \"Impact of Crime Rate on Median Home Values\",\n       x = \"Per Capita Crime Rate\",\n       y = \"Median Home Value ($1000's)\")\n```\n\n***`Exploratory Data Analysis (EDA) is a crucial step in understanding and gaining insights from a dataset. In the case of the Boston Housing Dataset, the summary statistics reveal the variability and distribution of key features such as crime rate (CRIM), proportion of residential land (ZN), and non-retail business acres (INDUS). The dataset contains information about factors like air quality (NOX), average number of rooms (RM), and socio-economic status (LSTAT), which may influence the median value of owner-occupied homes (MEDV). Handling missing values is essential, and in this dataset, 20 rows have missing entries in various columns. Through graphical representations such as histograms and scatter plots, relationships between variables can be observed, aiding in the identification of patterns and potential outliers. The dataset provides a comprehensive view of diverse factors influencing housing values, setting the stage for further in-depth analysis and modeling.`***\n\n***`The correlation matrix provides insights into the relationships between various features in the dataset. Examining the correlation matrix for the Boston Housing Dataset, it is evident that certain pairs of variables exhibit notable correlations. For instance, the variable \"RM,\" representing the average number of rooms per dwelling, displays a positive correlation with the target variable \"MEDV,\" indicating a higher median home value with an increased number of rooms. Conversely, the \"LSTAT\" variable, representing the percentage of the lower status of the population, demonstrates a negative correlation with \"MEDV,\" suggesting that areas with a higher percentage of lower-status residents tend to have lower median home values. These insights from the correlation matrix lay the foundation for further exploration and model building, guiding the selection of relevant features for predictive analysis in the Boston Housing Dataset.`***\n\n------------------------------------------------------------------------\n\n```{r}\n# Install and load the required packages\nlibrary(randomForest)\nlibrary(ggplot2)\n\n# Load the dataset\ndata <- read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\")\n\n# Remove rows with missing values\ndata <- na.omit(data)\n\n# Separate features (X) and target variable (y)\nX <- data[, colnames(data) != \"MEDV\"]\ny <- data$MEDV\n\n# Split the data into training and testing sets\nset.seed(42)\nindices <- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data <- data[indices, ]\ntest_data <- data[-indices, ]\n\n# Initialize the Random Forest Regressor\nrf_model <- randomForest(MEDV ~ ., data = train_data, ntree = 100)\n\n# Make predictions on the test set\npredictions <- predict(rf_model, newdata = test_data)\n\n# Evaluate the model\nmse <- mean((predictions - test_data$MEDV)^2)\nr_squared <- 1 - mse / var(test_data$MEDV)\n\nprint(paste(\"Mean Squared Error:\", mse))\nprint(paste(\"R-squared:\", r_squared))\n\n# Visualization: Scatter plot of actual vs. predicted values\nggplot(data = test_data, aes(x = MEDV, y = predictions)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n```\n\n***The machine learning model, utilizing a Random Forest Regressor with 100 trees, demonstrated promising performance on the Boston Housing Dataset. The evaluation metrics indicate a mean squared error (MSE) of 10.24 and an R-squared value of 0.90 on the test set. The MSE measures the average squared difference between the predicted and actual median values of owner-occupied homes, providing a quantitative assessment of the model's accuracy. In this context, the relatively low MSE signifies that the model's predictions are, on average, close to the true values. Additionally, the high R-squared value of 0.90 suggests that the majority of the variance in the target variable is explained by the model, indicating a strong predictive capability. These results imply that the Random Forest Regressor has effectively captured the complex relationships within the dataset, providing a reliable framework for predicting median home values based on the given features.***\n\n------------------------------------------------------------------------\n\n```{r}\n# Install and load the required packages\nlibrary(pdp)\nlibrary(xgboost)\nlibrary(ggplot2)\n\n# Load the dataset\ndata <- read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\")\n\n# Remove rows with missing values\ndata <- na.omit(data)\n\n# Separate features (X) and target variable (y)\nX <- data[, colnames(data) != \"MEDV\"]\ny <- data$MEDV\n\n# Split the data into training and testing sets\nset.seed(42)\nindices <- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data <- data[indices, ]\ntest_data <- data[-indices, ]\n\n# Initialize the XGBoost Regressor\nxgb_model <- xgboost(data = as.matrix(train_data[, colnames(train_data) != \"MEDV\"]), label = train_data$MEDV, nrounds = 100, objective = \"reg:squarederror\")\n\n# Make predictions on the test set\npredictions <- predict(xgb_model, as.matrix(test_data[, colnames(test_data) != \"MEDV\"]))\n\n# Evaluate the model\nmse <- mean((predictions - test_data$MEDV)^2)\nr_squared <- 1 - mse / var(test_data$MEDV)\n\nprint(paste(\"Mean Squared Error:\", mse))\nprint(paste(\"R-squared:\", r_squared))\n\n# Visualization 1: Scatter plot of actual vs. predicted values\nggplot(data = test_data, aes(x = MEDV, y = predictions)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n# Visualization 2: Feature Importance Plot\nfeature_importance <- xgb.importance(model = xgb_model)\nxgb.plot.importance(importance_matrix = feature_importance)\n\n```\n\n***`The XGBoost model demonstrates a notable reduction in training root mean squared error (train-rmse) throughout the training process, starting from 16.86 and progressively decreasing to 0.0088. The mean squared error on the test set is 9.55, indicating strong predictive performance. The high R-squared value of 0.9045 further confirms the model's ability to explain the variance in the target variable, suggesting that the XGBoost model effectively captures the underlying patterns in the housing data, making it a robust choice for regression tasks.`***\n\n```{r}\nlibrary(xgboost)\nlibrary(randomForest)\nlibrary(ggplot2)\n\n# Load and preprocess the data\ndata <- na.omit(read.csv(\"C:/Users/prasad/Downloads/HousingData.csv\"))\nset.seed(42)\nindices <- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data <- data[indices, ]\ntest_data <- data[-indices, ]\n\n# Model A: XGBoost\nxgb_model <- xgboost(data = as.matrix(train_data[, colnames(train_data) != \"MEDV\"]), label = train_data$MEDV, nrounds = 100, objective = \"reg:squarederror\")\npredictions_xgb <- predict(xgb_model, as.matrix(test_data[, colnames(test_data) != \"MEDV\"]))\nmse_xgb <- mean((predictions_xgb - test_data$MEDV)^2)\n\n# Model B: Random Forest\nrf_model <- randomForest(MEDV ~ ., data = train_data, ntree = 100)\npredictions_rf <- predict(rf_model, newdata = test_data)\nmse_rf <- mean((predictions_rf - test_data$MEDV)^2)\n\n# Comparative Analysis\nprint(\"XGBoost Model Metrics:\")\nprint(paste(\"Mean Squared Error:\", mse_xgb))\n\nprint(\"Random Forest Model Metrics:\")\nprint(paste(\"Mean Squared Error:\", mse_rf))\n\n# Comparative Scatter Plots\nplot_xgb <- ggplot(data = test_data, aes(x = MEDV, y = predictions_xgb)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"XGBoost - Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\nplot_rf <- ggplot(data = test_data, aes(x = MEDV, y = predictions_rf)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Random Forest - Actual vs. Predicted Values\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n# Display Plots\nprint(\"Comparative Scatter Plots:\")\nprint(plot_xgb)\nprint(plot_rf)\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":{"light":"zephyr","dark":["superhero"]},"title-block-banner":true,"title":"Unveiling the Marvels of Data Science: A Journey into the Heart of Information","author":"Bhanu Varaprasad","date":"2023-12-08","categories":["news","code","analysis","plotly","plot"],"image":"profile.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}